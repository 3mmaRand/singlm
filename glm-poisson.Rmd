# (PART) POISSON GLMs {-}

# Poisson GLM overview {#pois-glm-overview}

When a response variable is the count of objects, individuals or events it often follows a Poisson distribution. Such variables are always positive - they range from 0 to $\infty$. A Poisson GLM is also known as Poisson regression. The link function used in a Poisson GLM is the natural logarithm, $ln$.

When you have a single explanatory variable, that model is:

\begin{equation}
ln(E(y_{i}))=\beta_{0}+\beta_{1}X1_{i}
(\#eq:glmpois)
\end{equation}

This means that the model estimates are logged to the base $e$ and and the inverse function, `exp()` must be applied to them to interpret them in terms of the response. In other words, to make predictions about the expected value of the response we need to exponentiate the coefficients.

\begin{equation}
E(y_{i})=exp(\beta_{0}+\beta_{1}X1_{i})
(\#eq:glmpois2)
\end{equation}

or

\begin{equation}
E(y_{i})=exp(\beta_{0}) \times exp(\beta_{1})^{X1_{i}}
(\#eq:glmpois3)
\end{equation}

Just like examples of general linear models with a single explanatory variable, there are two parameters in this model, $\beta_{0}$ and $\beta_{0}$ and their meaning is similar. $\beta_{0}$ is the log of the expected $y$ when $x$ is zero (i.e., the intercept). The log of $\beta_{1}$ is not the amount you *add* to $y$ for each unit change in $x$ but the amount by which to *multiply*. This means the model is a curve. If $\beta_{1}$ is positive, $exp(\beta_{1})$ is greater than one and $y$ increases as $x$ increases; if $\beta_{1}$ is negative, $exp(\beta_{1})$ is less than one and $y$ decreases as $x$ increases. See Figure \@ref(fig:glm-pois-poss) for an illustration of the curve for positive and negative $\beta_{1}$.

(ref:glm-pois-poss) Data fitted with a Poisson GLM.

```{r glm-pois-poss, echo = FALSE, fig.cap="(ref:glm-pois-poss)"}
n <- 12
# coefficients
beta0 <- .5
beta1 <- 0.25
# generate covariate values
set.seed(11)
x <- runif(n = n, min = 0, max = 8)
# compute mus
mu <- exp(beta0 + beta1 * x)
# generate Y-values
y <- rpois(n = n, lambda = mu)
# data set
df <- data.frame(y = y, x = x)
p1 <- df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(method = "glm", 
              method.args = list(family = "poisson"),
              se = FALSE,
              colour = "black") +
  annotate("text", x = 3, y = 9, 
           label = expression(beta[1]~is~positive),
           size = 7) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 8.5)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 12)) +
  theme_classic() +
  theme(axis.title = element_blank(),
        axis.text = element_blank())

# coefficients
beta0 <- 2.3
beta1 <- -0.25
set.seed(13)
x <- runif(n = n, min = 0, max = 8)
# compute mus
mu <- exp(beta0 + beta1 * x)
# generate Y-values
y <- rpois(n = n, lambda = mu)
# data set
df <- data.frame(y = y, x = x)
p2 <- df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(method = "glm", 
              method.args = list(family = "poisson"),
              se = FALSE,
              colour = "black") +
  annotate("text", x = 5, y = 9, 
           label = expression(beta[1]~is~negative),
           size = 7) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 8.5)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 12)) +
  theme_classic() +
  theme(axis.title = element_blank(),
        axis.text = element_blank())
p1 + p2
```


See Figure \@ref(fig:glm-pois-annotated) for a graphical representation of generalised linear model terms. 

(ref:glm-pois-annotated) A Generalised linear model with Poisson distributed errors. The measured `r kableExtra::text_spec("response values are in pink", color = pal3[2], bold = TRUE)`, the `r kableExtra::text_spec("predictions are in green", color = pal3[3], bold = TRUE)`, and the differences between these, known as the `r kableExtra::text_spec("residuals, are in blue", color = pal3[1], bold = TRUE)`. The estimated model parameters, $\beta_{0}$ and $\beta_{1}$ must be exponentiated to be interpreted on the scale of the response. When $x=0$ we predict the number of $y$ to be $exp(\beta_{0})$. For each unit of $x$, the number of $y$ changes by a factor of $exp(\beta_{1})$


```{r glm-pois-annotated, echo = FALSE, fig.cap="(ref:glm-pois-annotated)"} 
knitr::include_graphics("images/generic_pois.svg")
```

## When explanatory is categorical

If your response is a count and you just one categorical explanatory variable you do not need a Poisson GLMS. Use a chi-squared test

## More than one explanatory

\begin{equation}
ln(E(y_{i}))=\beta_{0}+\beta_{1}X1_{i}+\beta_{2}X2_{i}+...+\beta_{p}Xp_{i}
(\#eq:glmpois4)
\end{equation}

To make predictions about the expected value of the response we need to exponentiate the coefficients.

\begin{equation}
E(y_{i})=exp(\beta_{0}+\beta_{1}X1_{i}+\beta_{2}X2_{i}+...+\beta_{p}Xp_{i})
(\#eq:glmpois5)
\end{equation}


<!-- ### Checking assumptions -->
<!-- The assumptions of the model are checked using the `plot()` function which produces diagnostic plots to explore the distribution of the residuals. They are not proof of the assumptions being met but allow us to quickly determine if the assumptions are plausible, and if not, how the assumptions are violated and what data points contribute to the violation. -->

```{r echo=FALSE}
# n <- 100
# beta0 <- 2.3
# beta1 <- -0.28
# set.seed(13)
# x <- runif(n = n, min = 0, max = 15)
# # compute mus
# mu <- exp(beta0 + beta1 * x)
# # generate Y-values
# y <- rpois(n = n, lambda = mu)
# # data set
# df <- data.frame(y = y, x = x)
# mod <- glm(data = df, y ~ x, family = poisson)

```
<!-- The two plots which are most useful are  the "Residuals vs Fitted" plot (plot 1) and the "Q-Q" plot (plot 2). These are given as values to the `which` argument of `plot()`. -->


<!-- The Residuals vs Fitted plot shows if residuals have homogeneous variance or have non-linear patterns. Non-linear relationship between explanatory variables and the response will usually show in this plot if the model does not capture the non-linear relationship. For the assumptions to be met, the residuals should be equally spread around a horizontal line: -->
```{r echo=FALSE, out.width="60%"}
# plot(mod, which = 1)
```

<!-- The following are two examples in which the residuals do not have homogeneous variance and display non-linear patterns. -->
```{r echo=FALSE, out.width="60%"}
# set.seed(100)
# y <-  (b0 + b1 * x) + rexp(n,rate = 0.1)
# df <- data.frame(x = x, y = y)
# mod2 <- lm(data = df, x ~ y)
# plot(mod2, which = 1)
# plot(mod3, which = 1)
```

<!-- The Q-Q plot is a scatterplot of the residuals (standardised to a mean of zero and a standard deviation of 1) against what is expected if the residuals are normally distributed.  -->
```{r echo=FALSE, out.width="60%"}
# plot(mod, which = 2)
```

<!-- The following are two examples in which the residuals are not normally distributed. -->
```{r echo=FALSE, out.width="60%"}
# set.seed(100)
# y <-  (b0 + b1 * x) + rexp(n,rate = 5)
# df <- data.frame(x = x, y = y)
# mod2 <- lm(data = df, x ~ y)
# plot(mod2, which = 2)
```



```{r echo=FALSE, out.width="60%"}
# set.seed(1234)
# y <-  (b0 + b1 * x) + rpois(n,lambda = 1)
# df <- data.frame(x = x, y = y)
# mod3 <- lm(data = df, x ~ y)
# plot(mod3, which = 2)
```


## Reporting
The important information to include when reporting the results of fitting a Poisson GLM are the most notable predictions and the significance, direction and magnitude of effects. You need to ensure your reader will understand what the data are saying even if all the numbers and statistical information was removed. For example, variable $Y$ increase with variables $X1$. 

In relatively simple models, reporting group means or a slope, and statistical test information is enough. In more complex models with many variables is it common to give all the estimated model coefficients in a table.

In addition, your figure should show both the data and the model. This is honest and allows your interpretation to be evaluated. 