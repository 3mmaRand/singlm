[
["index.html", "singlm: A simple introduction to GLM for analysing Poisson and Binomial responses in R Preface 0.1 Who is this book for? 0.2 Approach of this book 0.3 Options on the toolbar 0.4 Conventions used in the book 0.5 Following along with the examples 0.6 Overview of the chapter contents 0.7 Software information", " singlm: A simple introduction to GLM for analysing Poisson and Binomial responses in R Emma Rand September 2020 Preface 0.1 Who is this book for? This book is for R users who have done an introductory class in data analysis which covered hypothesis testing and applying and interpreting linear models in R with the lm() function. I assume you are familiar with, but not expert in, using R and RStudio to import data, analyse it and interpret the results and create figures using ggplot(). The book aims to teach you how to use and interpret the glm() function in R for two types of response data which are are not normally distributed: Poisson distributed responses (counts) and binomially distributed responses (binary outcomes). 0.2 Approach of this book Models are explained with reference to examples. Each example demonstrates the R code needed, how understand the output and how to report the results, including suggested ggplot2figures. The code is given for figures but not extensively explained. To learn more go to https://ggplot2.tidyverse.org/ 0.3 Options on the toolbar You can change the appearance of the book using the toolbar at the top of the page. The menu on the left can be hidden, the font size increased or decreased and the colour altered to a dark or sepia theme. Search the book by clicking on the magnifying glass, entering a search term and using the up and down arrows to navigate through the results. 0.4 Conventions used in the book Code and any output appears in blocks formatted like this: cases &lt;- read_table2(&quot;data-raw/cases.txt&quot;) glimpse(cases) # Rows: 43 # Columns: 2 # $ cancers &lt;dbl&gt; 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1... # $ distance &lt;dbl&gt; 154.37, 93.14, 3.83, 60.83, 142.61, 164.72, 135.92, 79.92,... Lines of output start with a #. The content of a code block can be copied using the icon in its top right. Within the text: packages are indicated in bold code font like this: ggplot2 functions are indicated in code font with brackets after their name like this: ggplot() R objects are indicated in code font like this: cases Key points are summarised throughout the book using boxes like this: The key point of a previous few paragraphs is in boxes like these Extra pieces of information that are not essential to understanding the material are presented like this: Extra information and tips are in boxes like these 0.5 Following along with the examples Readers may wish to code along and the following gives guidance on how best to do that. I recommend starting a new RStudio project and creating a folder inside that project called data-raw where you will save the data files. Links to the data files are given in the text and these can be downloaded to your data-raw folder by right-clicking the link choosing the option to save. Then make a new script file for each example to carry our the analysis for that example. For example, if you call your Project singlm and you have just started Chapter 3, your folder structure would look like this: -- singlm |-- singlm.Rproj |-- cases_poisson.R |-- data-raw |-- cases.text Using this structure will mean the paths to files needed in your code are the same as those given in the book. I use packages from the tidyverse (Wickham et al. 2019) including ggplot2 (Wickham 2016), dplyr (Wickham et al. 2020), tidyr (Wickham 2020) and readr (Wickham, Hester, and Francois 2018) throughout the book. All the code assumes you have loaded the core tidyverse packages with: library(tidyverse) If you run examples and get an error like this: # Error in read_table2(&quot;data-raw/cases.txt&quot;) : # could not find function &quot;read_table2&quot; It is likely you need to load the tidyverse as shown above. All other packages will be loaded explicitly with library() statements where needed. 0.6 Overview of the chapter contents This book introduces the the Generalised Linear Model for two types of discrete response: 1.Poisson distributed: when a response variable is the number of things. 2. Binomially distributed: when a response variable can take one of only two values, such as “yes” or “no”, “alive” or “dead”, “present” or “absent”. In R, these are analysed with the glm() function. glm() can be used to perform tests using the Generalised Linear Model for response variables which are counts or binary. 0.7 Software information I used the knitr package (Xie 2015) and the bookdown package (Xie 2020) to compile this book. My R session information is shown below: sessionInfo() # R version 4.0.2 (2020-06-22) # Platform: x86_64-w64-mingw32/x64 (64-bit) # Running under: Windows 10 x64 (build 16299) # # Matrix products: default # # locale: # [1] LC_COLLATE=English_United Kingdom.1252 # [2] LC_CTYPE=English_United Kingdom.1252 # [3] LC_MONETARY=English_United Kingdom.1252 # [4] LC_NUMERIC=C # [5] LC_TIME=English_United Kingdom.1252 # # attached base packages: # [1] stats graphics grDevices utils datasets methods base # # other attached packages: # [1] patchwork_1.0.1 kableExtra_1.2.1 forcats_0.5.0 stringr_1.4.0 # [5] dplyr_1.0.2 purrr_0.3.4 readr_1.4.0 tidyr_1.1.2 # [9] tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 # # loaded via a namespace (and not attached): # [1] tidyselect_1.1.0 xfun_0.18 haven_2.3.1 colorspace_1.4-1 # [5] vctrs_0.3.4 generics_0.0.2 htmltools_0.5.0 viridisLite_0.3.0 # [9] yaml_2.2.1 utf8_1.1.4 blob_1.2.1 rlang_0.4.7 # [13] pillar_1.4.6 glue_1.4.1 withr_2.3.0 DBI_1.1.0 # [17] dbplyr_1.4.4 modelr_0.1.8 readxl_1.3.1 lifecycle_0.2.0 # [21] munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 rvest_0.3.6 # [25] evaluate_0.14 knitr_1.30 fansi_0.4.1 broom_0.7.1 # [29] Rcpp_1.0.5 scales_1.1.1 backports_1.1.9 webshot_0.5.2 # [33] jsonlite_1.7.1 fs_1.5.0 hms_0.5.3 digest_0.6.25 # [37] stringi_1.5.3 bookdown_0.20.6 grid_4.0.2 cli_2.0.2 # [41] tools_4.0.2 magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 # [45] ellipsis_0.3.1 xml2_1.3.2 reprex_0.3.0 lubridate_1.7.9 # [49] assertthat_0.2.1 rmarkdown_2.4.1 httr_1.4.2 rstudioapi_0.11 # [53] R6_2.4.1 compiler_4.0.2 References "],
["what-are-glms-1.html", "Chapter 1 What are GLMs? 1.1 Overview 1.2 Model fitting 1.3 Types of GLM 1.4 More than one explanatory variable 1.5 Generalised linear models in R", " Chapter 1 What are GLMs? 1.1 Overview General Linear models predict the value of a normally distributed response from a linear combination of predictor variables. The Generalised Linear Model is an extension of the General Linear model for situations where the response variable does not follow the normal distribution. It can, instead, come from a large family of distributions known as the Exponential family which includes the normal, Poisson and binomial distributions. It generalises by allowing the linear model to be related to the response variable by a something we call a link function. When you have a single explanatory variable, the General Linear model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i} \\tag{1.1} \\end{equation}\\] Where: \\(y\\) is the response variable and \\(X1\\) is the explanatory variable. \\(i\\) is the index so \\(X1_{i}\\) is the \\(i\\)th value of \\(X1\\) \\(E(y_{i})\\) is the expected value of \\(y\\) for the \\(i\\)th value of \\(X1\\). \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the coefficients in the model. And that for the Generalised Linear Model is: \\[\\begin{equation} function(E(y_{i}))=\\beta_{0}+\\beta_{1}X1_{i} \\tag{1.2} \\end{equation}\\] Where: \\(function()\\) is the link function The link function is a transformation applied to the expected value of the response to link it to the explanatory variables. If you measured the response for a particular value of \\(x\\) very many times there would be some distribution of those responses. Instead of that distribution having to be normal, it can come from a different distribution such as the Poisson or Binomial distributions. As a consequence, the residuals also come from that distribution. The fact that the residuals can follow a distribution other than normal also means the the variance no longer has to be homogeneous but can change for the values of x. You can see that the two model equations are similar in structure. This is with good reason - you can think of the general linear model as a special case of generalised linear model when no transformation to the expected value, \\(E(y_{i})\\), is required. 1.2 Model fitting A very important difference between the general linear model (applied with lm()) and the generalised linear model (applied with glm()) is that the estimates of the coefficients are given on the scale of the link function. You cannot just read the predicted response at \\(x=0\\) from \\(\\beta_{0}\\) - you have to invert the link function to express the coefficients in terms of the response. Another important difference between these models is in the measure of fit and the test on that measure of fit. The parameters of General linear models are chosen to minimise the sum of the squared residuals and use \\(R^2\\), the proportion of variance explained. A variance ratio test, the F-test, is used to determine if the proportion of variance explained is significant. In GLMS we maximise the log-likelihood of our model (\\(l\\)) to choose our parameter values. This is known as maximum likelihood estimation. The measure of fit used in GLMs is deviance where deviance is \\(-2l\\). Low deviance means a good fit and high deviance a worse fit. Thus maximum likelihood estimation can be thought of as minimising deviance. The test again compares the deviance of predictions from the model with deviance in an intercept only model. This deviance in predictions from the intercept alone is called the Null deviance. Instead of considering the size of the \\(R^2\\) we consider the reduction in deviance between the null model and the residual deviance (the deviance left over after the model fit). You can consider the residual deviance to play the same role as \\(SSE\\). The difference between the null model deviance and the residual deviance of the full model has a chi-squared distribution and the test of whether the model is good overall is a “chi-squared test of deviance” This is analogous to the way that general linear model uses the \\(F\\) variance ratio test. The residual deviance plays the same role in GLMs that \\(SSE\\) plays in the general linear model. It is how much ‘variation’ is unexplained by the model The Akaike Information Criterion (AIC) is measure of fit that plays the same role in GLMs that adjusted \\(R^2\\) plays in the general linear model. It is trade-off between the goodness of fit of the model and the simplicity of the model. AIC values can also be determined for general linear models (but adjusted \\(R^2\\) cannot be calculated for GLMs). Figure 1.1: Hirotugu Akaike. By The Institute of Statistical Mathematics - The Institute of Statistical Mathematics, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64389664 1.3 Types of GLM Poisson GLMs are used when our response is a count. They are also known as GLMs with Poisson errors or Poisson regression. The link function is the natural logarithm, \\(ln\\), a function you probably know. This means the predictions are log counts and the coefficients have to be exponentiated using exp() to get predicted counts because exp() is the inverse of log(). In R, the log() function computes natural logarithms (i.e., logs to the base \\(e\\) by default. Binomial GLMs are used when our response is binary, a zero or one, or a proportion. For example, the response could be died (0) or survived (1), absent (0) or present (1) or right-handed (0) or left-handed (1). Binomial GLMs are also known as GLMs with binomial errors, binomial regression or logistic regression. The link function is \\(logit\\), a function you may not have heard of before. The predictions are “log-odds” and the coefficients have to be exponentiated using exp() and interpreted as odds. Often the easiest way to understand the predictions is to use the predict() function to get predictions on the scale of the response. If the binomial variable is died (0) or survived(1) these will be probabilities of survival. 1.4 More than one explanatory variable When you have more than one explanatory variable these are given as \\(X2\\), \\(X3\\) and so on up to the \\(p\\)th explanatory variable. Each explanatory variable has its own \\(\\beta\\) coefficient. The general form of the model is: \\[\\begin{equation} function(E(y_{i}))=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+...+\\beta_{p}Xp_{i} \\tag{1.3} \\end{equation}\\] 1.5 Generalised linear models in R 1.5.1 Building and viewing Poisson and binomial generalised linear models (and others) can be carried out with the glm() function in R. It uses the same method for specifying the model that other functions including lm() use. When you have one explanatory variable the command is: glm(data = dataframe, response ~ explanatory, family = distribution(link = linkfunction)) For a Poisson GLM this is: glm(data = dataframe, response ~ explanatory, family = poisson(link = log)) For a binomial GLM this is: glm(data = dataframe, response ~ explanatory, family = binomial(link = “logit”)) The model formula can be developed in the same way we’ve seen previously. When you have two explanatory variable we add the second explanatory variable to the formula using a + or a *. The command is: glm(data = dataframe, response ~ explanatory1 + explanatory2, family = distribution(link = linkfunction)) or glm(data = dataframe, response ~ explanatory1 * explanatory2, family = distribution(link = linkfunction)) A model with explanatory1 + explanatory2 considers the effects of the two variables independently. A model with explanatory1 * explanatory2 considers the effects of the two variables and any interaction between them. We usually assign the output of glm() commands to an object and view it with summary(). The typical workflow would be: mod &lt;- glm(data = dataframe, response ~ explanatory, family = distribution(link = linkfunction)) summary(mod) glm() can be used to perform tests using the Generalised Linear Model including Poisson and Binomial regression. Elements of the glm() object include the estimated coefficients can be accessed with mod$coeffients. 1.5.2 Getting predictions mod$fitted.values gives the predicted values for the explanatory variable values actually used in the experiment, i.e., there is a prediction for each row of data. These are given on the scale of the response. This means they will be predicted counts for Poisson GLMs and predicted probabilities for Binomial GLMs. 1.5.3 Tests of the model With an lm() you get an F variance ratio test of the model over all when you view a summary. It answers the question “does the model explain a significant amount of the variance in the response relative to the response mean?” With a glm() you have to ask for a chi-squared test of deviance. It answers the question “does the model significantly decrease the deviance relative to the response mean?” anova(mod, test = “Chisq”) To get predictions for a different set of values you need to make a dataframe of the different set of values and use the predict() function. When using the predict() function we have to specify that we want our predictions on the scale of the response rather than the scale of the link function using the type argument. The typical workflow would be: predict_for &lt;- data.frame(explanatory = values) predict_for$pred &lt;- predict(mod, newdata = predict_for, type = “response”) "],
["pois-glm-overview-2.html", "Chapter 2 Poisson GLM overview 2.1 When explanatory is categorical 2.2 More than one explanatory 2.3 Reporting", " Chapter 2 Poisson GLM overview When a response variable is the count of objects, individuals or events it often follows a Poisson distribution. Such variables are always positive - they range from 0 to \\(\\infty\\). A Poisson GLM is also known as Poisson regression. The link function used in a Poisson GLM is the natural logarithm, \\(ln\\). When you have a single explanatory variable, that model is: \\[\\begin{equation} ln(E(y_{i}))=\\beta_{0}+\\beta_{1}X1_{i} \\tag{2.1} \\end{equation}\\] This means that the model estimates are logged to the base \\(e\\) and and the inverse function, exp() must be applied to them to interpret them in terms of the response. In other words, to make predictions about the expected value of the response we need to exponentiate the coefficients. \\[\\begin{equation} E(y_{i})=exp(\\beta_{0}+\\beta_{1}X1_{i}) \\tag{2.2} \\end{equation}\\] or \\[\\begin{equation} E(y_{i})=exp(\\beta_{0}) \\times exp(\\beta_{1})^{X1_{i}} \\tag{2.3} \\end{equation}\\] Just like examples of general linear models with a single explanatory variable, there are two parameters in this model, \\(\\beta_{0}\\) and \\(\\beta_{0}\\) and their meaning is similar. \\(\\beta_{0}\\) is the log of the expected \\(y\\) when \\(x\\) is zero (i.e., the intercept). The log of \\(\\beta_{1}\\) is not the amount you add to \\(y\\) for each unit change in \\(x\\) but the amount by which to multiply. This means the model is a curve. If \\(\\beta_{1}\\) is positive, \\(exp(\\beta_{1})\\) is greater than one and \\(y\\) increases as \\(x\\) increases; if \\(\\beta_{1}\\) is negative, \\(exp(\\beta_{1})\\) is less than one and \\(y\\) decreases as \\(x\\) increases. See Figure 2.1 for an illustration of the curve for positive and negative \\(\\beta_{1}\\). Figure 2.1: Data fitted with a Poisson GLM. See Figure 2.2 for a graphical representation of generalised linear model terms. Figure 2.2: A Generalised linear model with Poisson distributed errors. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) must be exponentiated to be interpreted on the scale of the response. When \\(x=0\\) we predict the number of \\(y\\) to be \\(exp(\\beta_{0})\\). For each unit of \\(x\\), the number of \\(y\\) changes by a factor of \\(exp(\\beta_{1})\\) 2.1 When explanatory is categorical If your response is a count and you just one categorical explanatory variable you do not need a Poisson GLMS. Use a chi-squared test 2.2 More than one explanatory \\[\\begin{equation} ln(E(y_{i}))=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+...+\\beta_{p}Xp_{i} \\tag{2.4} \\end{equation}\\] To make predictions about the expected value of the response we need to exponentiate the coefficients. \\[\\begin{equation} E(y_{i})=exp(\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+...+\\beta_{p}Xp_{i}) \\tag{2.5} \\end{equation}\\] 2.3 Reporting The important information to include when reporting the results of fitting a Poisson GLM are the most notable predictions and the significance, direction and magnitude of effects. You need to ensure your reader will understand what the data are saying even if all the numbers and statistical information was removed. For example, variable \\(Y\\) increase with variables \\(X1\\). In relatively simple models, reporting group means or a slope, and statistical test information is enough. In more complex models with many variables is it common to give all the estimated model coefficients in a table. In addition, your figure should show both the data and the model. This is honest and allows your interpretation to be evaluated. "],
["pois-glm-single-cont.html", "Chapter 3 Single continuous explanatory 3.1 Introduction to the example 3.2 Applying and interpreting glm() 3.3 Getting predictions from the model 3.4 Creating a figure 3.5 Reporting the results", " Chapter 3 Single continuous explanatory 3.1 Introduction to the example The number of cases of cancer reported by a clinic and its distance, in kilometres, from a nuclear plant were recorded and the data are in cases.txt. Researchers wanted to know if proximity to the nuclear power plant influenced the incidence of cancer. Bear in mind this is not great epidemiology - there would be very many other factors influencing the occurrence and reporting of cancer cases at a clinic. cancers distance 0 154.37 0 93.14 4 3.83 0 60.83 0 142.61 0 164.72 0 135.92 1 79.92 0 112.71 0 101.76 2 59.62 0 128.07 2 17.17 1 24.81 1 103.42 0 112.70 0 143.96 1 48.77 1 82.20 1 57.53 1 12.75 0 64.47 1 68.78 1 133.40 0 98.94 0 40.87 0 151.82 4 35.15 0 97.10 0 131.44 0 102.02 0 116.77 1 28.79 0 52.63 2 23.15 1 68.13 0 146.93 1 87.98 0 147.30 1 132.67 0 164.21 1 72.67 0 22.81 There are 2 variables: the response, cancers, is the number of cancer cases reported at a clinic and distance, gives the clinic’s distance from the nuclear plant. We will import the data with the read_table2() function and plot it with ggplot(). cases &lt;- read_table2(&quot;data-raw/cases.txt&quot;) # a default scatter plot of the data ggplot(data = cases, aes(x = distance, y = cancers)) + geom_point() Most of the clinics reporting no cases seem to be more distance from the nuclear plant and those reporting the highest numbers are within 50km. 3.2 Applying and interpreting glm() We build a generalised linear model of the number of cases explained by the distance with the glm() function as follows: mod &lt;- glm(data = cases, cancers ~ distance, family = poisson) Printing mod to the console gives us the estimated model parameters: mod # # Call: glm(formula = cancers ~ distance, family = poisson, data = cases) # # Coefficients: # (Intercept) distance # 1.0192 -0.0215 # # Degrees of Freedom: 42 Total (i.e. Null); 41 Residual # Null Deviance: 54.5 # Residual Deviance: 31.8 AIC: 78.2 We will postpone discussing the information in the last three lines until we view the model summary. \\(\\beta_{0}\\) is labelled “(Intercept)” and \\(\\beta_{1}\\) is labelled “distance”. Thus the equation of the line is: \\(ln(cancers)\\) = 1.019 \\(+\\) -0.021\\(\\times distance\\) The fact that the estimate for distance (-0.021) is negative tells us that as distance increases, the number of cancers reported goes down. These estimates are on the scale of the link function, that is, they are logged (to the base e, natural logs) in this case. To understand the parameters the on the scale of the response we apply the inverse of the \\(ln\\) function, the exp() function exp(mod$coefficients) # (Intercept) distance # 2.771 0.979 So the equation of the model is: \\(cancers\\) = 2.771 \\(\\times\\) 0.979\\(^{distance}\\) The model predicts there will be 2.771 cancers at a clinic at 0 km from the power plant. Recall that for a linear model with one predictor, the second estimate is the amount added to the intercept when the predictor changes by one value. Since this is GLM with a log link, the value of 0.979 is amount the intercept is multiplied by for each unit increase of distance. Thus the model predicts there will be 2.771 \\(\\times\\) 0.979 = 2.712 cancers 1 km away and 2.771 \\(\\times\\) 0.979 \\(\\times\\) 0.979 = 2.654 cancers 2 km away. That is: \\(\\beta_{0}\\) \\(\\times\\) \\(\\beta_{0}^n\\) mm at \\(n\\) km away. You can work these out either by exponentiating the coefficients and then multiplying the results or by adding the coefficients and exponentiating. Exponentiate coefficients then multiply: # 1km away exp(b0) * exp(b1) # [1] 2.71 # 2km away exp(b0) * exp(b1) * exp(b1) # [1] 2.65 # 10km away exp(b0) * exp(b1)^10 # [1] 2.23 Add the coefficients then exponentiate the sum: # 1km away exp(b0 + b1) # [1] 2.71 # 2km away exp(b0 + b1 + b1) # [1] 2.65 # 10km away exp(b0 + 10*b1) # [1] 2.23 Usually, we use the predict() function to make predictions for particular distances (see later). More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # glm(formula = cancers ~ distance, family = poisson, data = cases) # # Deviance Residuals: # Min 1Q Median 3Q Max # -1.842 -0.744 -0.483 0.421 1.893 # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) 1.01917 0.30871 3.30 0.00096 *** # distance -0.02150 0.00503 -4.27 2e-05 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # (Dispersion parameter for poisson family taken to be 1) # # Null deviance: 54.522 on 42 degrees of freedom # Residual deviance: 31.790 on 41 degrees of freedom # AIC: 78.16 # # Number of Fisher Scoring iterations: 5 The Coefficients table gives the estimated \\(\\beta_{0}\\) and \\(\\beta_{1}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated value for the intercept is 1.019 \\(\\pm\\) 0.309 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated value for the slope is -0.021 \\(\\pm\\) 0.005, also differs significantly from zero (\\(p\\) &lt; 0.001). Towards the bottom of the output there is information about the model fit. The null deviance (what exists if we predict the number of cases from an intercept, \\(\\beta_{0}\\), only model) is 54.522 with 42 degrees of freedom and the residual deviance (left over after our model is fitted) is 31.79 with 41 \\(d.f.\\). The model fits a 1 parameter, \\(\\beta_{1}\\), and thus accounts for 1 \\(d.f.\\) for a reduction in deviance by 22.732. To get a test of whether the reduction in deviance is significant for each term in the the model formula we use: To get a test of the model overall: anova(mod, test = &quot;Chisq&quot;) # Analysis of Deviance Table # # Model: poisson, link: log # # Response: cancers # # Terms added sequentially (first to last) # # # Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) # NULL 42 54.5 # distance 1 22.7 41 31.8 1.9e-06 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is a significant reduction in deviance for our model (p &lt; 0.001). 3.3 Getting predictions from the model The predict() function returns the predicted values of the response. To add a column of predicted values to the dataframe: we need to specify they should be on the scale of the responses, not on the link function scale. cases$pred &lt;- predict(mod, type = &quot;response&quot;) This gives predictions for the actual \\(x\\) values used. If you want predicts for other values of \\(x\\) you need to creating a data frame of the \\(x\\) values from which you want to predict For example, to predict for distances from 0 to 180 km in steps of 10 km: predict_for &lt;- data.frame(distance = seq(0, 180, 10)) predict_for$pred &lt;- predict(mod, newdata = predict_for, type = &quot;response&quot;) #plot(mod, which = 2) #plot(mod, which = 1) 3.4 Creating a figure ggplot(data = cases, aes(x = distance, y = cancers)) + geom_point() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;), se = FALSE, colour = &quot;black&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 190), name = &quot;Distance (km) of clinic from plant&quot;) + scale_y_continuous(expand = c(0, 0.03), limits = c(0, 5), name = &quot;Number of reported cancers&quot;) + theme_classic() 3.5 Reporting the results The number of cases reported by a clinic significantly decreases by a factor of 2.771 \\(\\pm\\) 0.005 for each kilometre from the nuclear plant (p &lt; 0.001). See figure 3.1. For a clinic at the plant, 1.019 \\(\\pm\\) 0.309 cases are expected Figure 3.1: Incidence of cancer cases reported at clinic by it distance from the nuclear plant. The line gives predictions for a GLM with Poisson distributed errors, \\(y\\) = 2.771 \\(\\times\\) 0.979\\(^{x}\\). "],
["pois-glm-two-cont.html", "Chapter 4 Two explanatory variables 4.1 Introduction to the example 4.2 Applying and interpreting glm() 4.3 Getting predictions from the model 4.4 Creating a figure 4.5 Reporting the results", " Chapter 4 Two explanatory variables 4.1 Introduction to the example The number of insect prey caught by individuals of a particular bird species varies. In an effort to understand this variation, researchers recorded the number of prey an individual caught, its age (in years) and how it spent the majority of it’s time (as a single individual, in a pair or in a group of many). The data are in birds.txt. prey age group 5 2.6 single 5 5.7 single 5 2.7 single 3 3.1 single 2 3.9 single 3 6.0 single 4 5.2 single 2 3.9 single 4 6.7 single 2 5.5 single 22 7.0 pair 12 4.4 pair 19 5.3 pair 23 4.4 pair 17 4.4 pair 14 3.4 pair 22 5.9 pair 21 5.1 pair 21 4.2 pair 30 5.5 pair 54 4.3 many 78 5.4 many 69 5.2 many 78 4.9 many 50 3.3 many 61 4.9 many 99 6.3 many 108 6.1 many 40 2.6 many 32 3.2 many There are 3 variables: the response, prey, is the number of caught by an individual; age, gives the individual’s age in years (to one tenth of a year); and group indicates how the individuals spent the majority of its time. We will import the data with the read_table2() function and plot it with ggplot(). birds &lt;- read_table2(&quot;data-raw/birds.txt&quot;) We need to be able to show the different group behaviours # a default scatter plot of the data ggplot(data = birds, aes(x = age, y = prey, colour = group)) + geom_point() Individuals that spend most of their time with many individuals catch more prey and this effect is increased with age. Individuals that spend most of their time alone do not seem to improve with age. 4.2 Applying and interpreting glm() We build a generalised linear model of the number of cases explained by the distance with the glm() function as follows: mod &lt;- glm(data = birds, prey ~ group * age, family = poisson) Printing mod to the console gives us the estimated model parameters: mod # # Call: glm(formula = prey ~ group * age, family = poisson, data = birds) # # Coefficients: # (Intercept) grouppair groupsingle age # 2.794 -0.416 -1.406 0.293 # grouppair:age groupsingle:age # -0.168 -0.323 # # Degrees of Freedom: 29 Total (i.e. Null); 24 Residual # Null Deviance: 849 # Residual Deviance: 21 AIC: 172 \\(\\beta_{0}\\) is labelled “(Intercept)” and \\(\\beta_{1}\\) to \\(\\beta_{5}\\) are labelled “grouppair”, “groupsingle”, “age”, “grouppair:age” and “groupsingle:age” Thus the equation of the line is: \\(ln(prey)\\) = 2.794 \\(+\\) -0.416\\(\\times grouppair\\) \\(+\\) -1.406\\(\\times groupsingle\\) \\(+\\) 0.293\\(\\times age\\) \\(+\\) -0.168\\(\\times grouppair:age\\) \\(+\\) -0.323\\(\\times groupsingle:age\\) The intercept is the log of the expected number of prey items caught by 0 aged birds that spend the majority of their time in a group of many. The fact that the estimate for grouppair (-0.416) is negative tells us that those in pairs aged 0 catch fewer. Aged 0 single birds also catch fewer. The positive coefficient for age indicates that more prey are caught with age. To understand the parameters the on the scale of the response we apply the inverse of the \\(ln\\) function, the exp() function exp(mod$coefficients) # (Intercept) grouppair groupsingle age grouppair:age # 16.338 0.660 0.245 1.340 0.845 # groupsingle:age # 0.724 So: \\(prey\\) = 16.338 \\(\\times\\) 0.66\\(^{grouppair}\\) \\(\\times\\) 0.245\\(^{groupsingle}\\) \\(\\times\\) 0.66\\(^{grouppair}\\) \\(\\times\\) 0.845\\(^{grouppair:age}\\) \\(\\times\\) 0.724\\(^{groupsingle:age}\\) Birds aged 0 that spend most of their time with many others are expected to catch 16.338 prey. The paired birds aged 0 are expected to catch 16.338 \\(\\times\\) 0.66 = 10.778 prey and the lone birds aged 0, 16.338 \\(\\times\\) 0.245 = 10.778 prey. Birds that spend most of their time with many others catch 1.34 times more prey with each year that they age. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # glm(formula = prey ~ group * age, family = poisson, data = birds) # # Deviance Residuals: # Min 1Q Median 3Q Max # -1.640 -0.667 -0.172 0.650 1.767 # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) 2.7935 0.1813 15.41 &lt;2e-16 *** # grouppair -0.4161 0.4078 -1.02 0.308 # groupsingle -1.4059 0.5949 -2.36 0.018 * # age 0.2927 0.0354 8.27 &lt;2e-16 *** # grouppair:age -0.1685 0.0789 -2.13 0.033 * # groupsingle:age -0.3226 0.1260 -2.56 0.010 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # (Dispersion parameter for poisson family taken to be 1) # # Null deviance: 848.806 on 29 degrees of freedom # Residual deviance: 21.048 on 24 degrees of freedom # AIC: 171.8 # # Number of Fisher Scoring iterations: 4 The Coefficients table gives the estimated \\(\\beta s\\) again but this time with their standard errors and tests of whether the estimates differ from zero. For example, The estimated value for the intercept is 2.794 \\(\\pm\\) 0.181 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated value for \\(\\beta_{1}\\) is -0.416 \\(\\pm\\) 0.408 does not differ significantly from zero (\\(p\\) = 0.308). At age 0, birds in pairs do not catch significantly fewer prey. Towards the bottom of the output there is information about the model fit. The null deviance (what exists if we predict the number of prey from the mean of birds in groups of many aged 0, \\(\\beta_{0}\\)) is 848.806 with 29 degrees of freedom and the residual deviance (left over after our model is fitted) is 21.048 with 24 \\(d.f.\\). The model fits a five parameters and thus accounts for 5 \\(d.f.\\) for a reduction in deviance by 827.758. To get a test of whether the reduction in deviance is significant for each term in the the model formula we use: anova(mod, test = &quot;Chisq&quot;) # Analysis of Deviance Table # # Model: poisson, link: log # # Response: prey # # Terms added sequentially (first to last) # # # Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) # NULL 29 849 # group 2 752 27 97 &lt; 2e-16 *** # age 1 66 26 31 4.1e-16 *** # group:age 2 10 24 21 0.0071 ** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All three terms in the model significantly reduce the deviance: group (\\(p\\) &lt; 0.001), age (\\(p\\) &lt; 0.001) and the interaction between them (\\(p\\) = 0.007). This means that group size matters, age matters and the effect of age is not the same for individuals in groups of different sizes. 4.3 Getting predictions from the model The predict() function returns the predicted values of the response. To add a column of predicted values to the dataframe: we need to specify they should be on the scale of the responses, not on the scale of the link function. birds$pred &lt;- predict(mod, type = &quot;response&quot;) This gives predictions for the ages used. If you want predictions for other ages you need to create a data frame of the values from which you want to predict To predict for ages 0, mean(age) and 7 for each group size we can use: predict_for &lt;- data.frame(group = rep(c(&quot;many&quot;, &quot;pair&quot;, &quot;single&quot;), each = 3), age = rep(c(c(0, mean(birds$age), 7)), times = 3)) We want predictions for three ages so need to repeat the group name each three times. We repeat the list of ages three times because there are three group sizes. The result is: group age many 0.0 many 4.7 many 7.0 pair 0.0 pair 4.7 pair 7.0 single 0.0 single 4.7 single 7.0 We then specify this dataframe in the predict() function using the newdata argument predict_for$pred &lt;- predict(mod, newdata = predict_for, type = &quot;response&quot;) group age pred many 0.0 16.34 many 4.7 64.71 many 7.0 126.73 pair 0.0 10.78 pair 4.7 19.33 pair 7.0 25.70 single 0.0 4.00 single 4.7 3.48 single 7.0 3.25 This sort of information is helpful in explaining our results. 4.4 Creating a figure ggplot(data = birds, aes(x = age, y = prey, colour = group)) + geom_point() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;), se = FALSE) + scale_color_manual(values = pal4, name = &quot;Group size&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 7.5), name = &quot;Age (years)&quot;) + scale_y_continuous(expand = c(0, 0.03), limits = c(0, 110), name = &quot;Number of prey caught&quot;) + theme_classic() 4.5 Reporting the results There is a significant effect of group size (p &lt; 0.001) on the number of prey items caught with averaged aged birds catching 64.7 prey if they spend their time in large groups, 19.3 prey for paired birds and only 3.5 prey for lone birds. There is also a significant effect of age (p &lt; 0.001) overall but this varies for birds in different group sizes (p = 0.007). Birds in large groups improve by a factor of 1.34 for each year, paired birds by a factor of 1.132 and single birds not at all. See figure 4.1. Figure 4.1: The effect of age and group size on the number of prey items caught by an individual. The line gives predictions for a GLM with Poisson distributed errors. "],
["bino-glm-overview-5.html", "Chapter 5 Binomial GLM overview", " Chapter 5 Binomial GLM overview When a response variable can take only one of two values such as 0 and 1, dead and alive, no and yes or failure and success it follows a binomial distribution and can be modelled with a binomial GLM. A binomial GLM is also known as logistic regression. Although the observations themselves can have only one of two values, the scale of the response is a probability representing the probability of 1, alive, yes or success. The link function used in a binomial GLM is the “logit” function also known as “log-odds”. This means the model estimates are also log-odds. This can make it tricky to interpret the estimates but two things can help us. First, the principle about the sign of \\(\\beta_{1}\\) still applies: positive \\(\\beta_{1}\\) indicates an increase in the response with an increase in \\(X1\\) and negative \\(\\beta_{1}\\) indicates a decrease in the response with an increase in \\(X1\\). Second, we can use the predict() function to get values on the scale of the response, a probability, rather than the log-odds When you have a single explanatory variable, that model is: \\[\\begin{equation} ln\\left(\\frac{E(y_{i})}{1-E(y_{i})}\\right)=\\beta_{0}+\\beta_{1}X1_{i} \\tag{5.1} \\end{equation}\\] This means that the model estimates are odds logged to the base \\(e\\) and and the inverse function, exp() must be applied to them to interpret as odds. \\[\\begin{equation} \\frac{E(y_{i})}{1-E(y_{i})}=exp(\\beta_{0}+\\beta_{1}X1_{i}) \\tag{5.2} \\end{equation}\\] or \\[\\begin{equation} \\frac{E(y_{i})}{1-E(y_{i})}=exp(\\beta_{0}) \\times exp(\\beta_{1})^{X1_{i}} \\tag{5.3} \\end{equation}\\] This looks complicated. Using the predict function because important in reporting the results. Just like Poisson GLM with a single explanatory variable, there are two parameters in this model, \\(\\beta_{0}\\) and \\(\\beta_{0}\\) and their meaning is similar. Let us assume our response is failure or success. Then \\(\\beta_{0}\\) is the log odds of success when the explanatory variable is 0 and \\(\\beta_{0}\\) is the change in the log odds of success for each unit change in the explanatory. What exactly are odds? An odds is the probability of one outcome divided by the probability of not that outcome. For example, for a fair coin where heads and tails are equally likely, the odds of a head in a coin toss is 1 because: \\(\\frac{P(H)}{P( not\\:head)} = \\frac{0.5}{ 1- 0.5} = 1\\) If you had an unfair coin where the probability of a head was \\(\\frac{3}{4}\\) then the odds of a head are: \\(\\frac{P(H)}{P( not\\:head)} = \\frac{0.75}{ 1- 0.75} = 3\\) A head is three times as likely as a tail. If instead probability of a head was \\(\\frac{1}{4}\\) then the odds of a head are: \\(\\frac{P(H)}{P( not\\:head)} = \\frac{0.25}{ 1- 0.25} = 0.33333\\) A head is a third as likely as a tail. Figure 5.1: Data fitted with a binomial GLM. See Figure ?? for a graphical representation of generalised linear model terms. For the binomial families the response can be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor not having the first level (and hence usually of having the second level). As a numerical vector with values between 0 and 1, interpreted as the proportion of successful cases (with the total number of cases given by the weights). As a two-column integer matrix: the first column gives the number of successes and the second the number of failures. "],
["summary.html", "Chapter 6 Summary", " Chapter 6 Summary what these models have in common: Responses must be independent key points where to go next "],
["references.html", "References", " References "]
]
