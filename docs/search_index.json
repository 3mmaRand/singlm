[
["index.html", "singlm: A simple introduction to GLM for analysing Poisson and Binomial responses in R Introduction 0.1 Who is this book for? 0.2 formatting options on the menu 0.3 Conventions used in the book 0.4 Overview of the chapter contents 0.5 Introductory class revision 0.6 Approach of this book", " singlm: A simple introduction to GLM for analysing Poisson and Binomial responses in R Emma Rand 2020-07-24 Introduction 0.1 Who is this book for? The aim of this book is to give people who have a little experience of doing data analysis in R a light introduction to Generalised Linear Models. It might be for you have done an introductory class in data analysis which covered classical univariate tests such as single linear regression, t-tests, one-way ANOVA and two-way ANOVA. It assumes you have some familiarity with R and RStudio and could import data, apply t.test() and aov() functions appropriately, interpret the results and create figures using ggplot(). It does not assume you are so fluent you could do these things with looking anything up, just that you would understand what you were doing and how to interpret the results. Secondary aim of this book is to introduce the terminology of statistical modelling to make your transition to more advanced texts easier. scope of the book, what isn’t covered Maths - intended to help you understand. ignore if it confuses you. 0.2 formatting options on the menu 0.3 Conventions used in the book With in the text Packages are indicated in bold code font like this: ggplot2 Functions are indicated in code font with brackets after their name like this: ggplot() The key point of a previous few paragraphs is in boxes like these Extra information and tips are in boxes like these objects in R are indicated in code font like this: mod stag we’ll be using tidyverse (Wickham et al. 2019) packages. You can learn the tidyverse 0.4 Overview of the chapter contents Chapter 1 In the first chapter we work through examples carried out in both lm() and their more more beginner friendly alternatives to gain a good understanding of the anatomy of lm() output. Chapter 2 … Chapter 3 … 0.5 Introductory class revision In experimental design and execution we manipulate or choose one or more variables and record how changing their values effect another variable. The variables we manipulate or choose are called explanatory or predictor variables and the other is called the response. These are also known as independent and dependent variables respectively. Predictor, Explanatory, x and Independent: all terms used to describe the variables we choose. Predicted, Response, y and Dependent: all terms used to describe the variable we measure. When we plot data, the response variable goes on the y-axis and the explanatory variable goes on the x-axis If we have two explanatory variables we might indicate the different values of one of them with colour. In choosing between regression, t-tests, one-way ANOVA and two-way ANOVA we consider how many explanatory variables we have and whether they are continuous or categorical. If we have one explanatory variable and it is continuous, we can apply a regression; if it is a categorical variable with two groups (or levels) we have the choice of a t-test or a one way ANOVA but when there are more than two groups we use a one-way ANOVA. A two-way ANOVA is used when there are two categorical explanatory variables. These apparently different tests are, in fact, the same test. They have the same underlying mathematics and, or to put it another way, the follow the same model. That model is usually known as the General Linear Model. Running a test = building or fitting a model. tests of how well our data fit the model, tests for the model parameters against a null hypothesis In R t-tests and ANOVA, like regression, can be carried out with the lm() function. The output differs but the results themselves are identical. The model makes a prediction for the response variable for a given value of the explanatory variable. The difference between the predicted value and the observed value is the residual. lm() can be used to perform tests using the General Linear Model including t-tests, ANOVA and regression for response variables which are normally distributed. The General Linear Model requires that the response variable has residuals that follow the normal distribution with variance which is homogeneous for the values of the explanatory variables. This commonly occurs when the response variable has a normal distribution. The General_ised_ Linear Model* extends the General Linear Model by including response variables that do not follow the normal distribution. This book introduces the the Generalised Linear Model for two types of response: Binomially distributed: response variables are binary, that is, they can take one of only two values, such as “yes” or “no”, “alive” or “dead”, “present” or “absent” Poisson distributed: response variables that indicate the number of things and thus take discrete values from 0 up. In R, these are analysed with the glm() function. glm() can be used to perform tests using the Generalised Linear Model for response variables which are counts or binary. 0.6 Approach of this book One of the reasons functions such as t.test() and aov() are taught rather than lm() is because the output is usually easier for those new to data analysis to understand and interpret. However, the output of lm() is more typical of statistical modelling functions in general and this makes it difficult for people to take small steps forward in their the statistical repertoire. The approach taken in this book is to exploit preexisting knowledge of t-tests and ANOVA using t.test() and aov() to understand the output of lm(). This will allow us to more easily understand the output of glm() References "],
["revisit.html", "Chapter 1 What are linear models 1.1 Introduction 1.2 What is a linear model? 1.3 Single linear regression 1.4 t-tests 1.5 One-way ANOVA 1.6 Two-way ANOVA 1.7 ", " Chapter 1 What are linear models 1.1 Introduction what is meant by a linear model revise regression revise t.tests doing t.tests as linear models linking output of t.test to lm revise one way ANOVA linking output of aov to lm revise two way ANOVA doing two way ANOVA as linear models linking output of aov to lm extensible - ancova design 1.2 What is a linear model? A linear model describes a continuous response variable as a function of one or more explanatory variables. When you have a single explanatory variable, that model is: \\[\\begin{equation} y_{i}=\\beta_{0}+\\beta_{1}X1_{i}+e_{i} \\tag{1.1} \\end{equation}\\] Where: The response variable is \\(y\\) and \\(X1\\) is the explanatory variable. \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the coefficients in the model. In a single linear regression, \\(\\beta_{0}\\) is often called the intercept and \\(\\beta_{1}\\) the slope. \\(i\\) is the index of the response so \\(y_{i}\\) is the \\(i\\)th response; if you had 20 pairs of \\(x\\)-\\(y\\) values, \\(i\\) would go from 1 to 20. \\(e_{i}\\) is the “error” also known as the residual. The equation means the response can be predicted from a given value of the explanatory variable, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) and will take that value plus some random noise. When you build a linear model from your data the procedure estimates the model coefficients. See figure 1.1. Figure 1.1: Terms used in linear models. keypoint terminology build fit parameter, coefficient estimates If you have more than one explanatory variable this these are given as \\(X2\\), \\(X3\\) and so on up to the \\(p\\)th explanatory variable each with its own \\(\\beta\\) coefficient. The general form of the model is: \\[\\begin{equation} y_{i}=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+...+\\beta_{p}XP_{i}+e_{i} \\tag{1.2} \\end{equation}\\] 1.3 Single linear regression 1.3.1 Introduction to the example This is a test you have probably carried out before. The concentration of juvenile hormone in stag beetles (Lucanus cervus) is known to influence mandible growth. Groups of stag beetles were injected with different concentrations of juvenile hormone (pg\\(\\mu\\)l-1) and their average mandible size (mm) determined. The data are in stag.txt. We will import the data with the read_table2() function from the readr package and plot it with ggplot() from the ggplot2 package. Both packages are part of the tidyverse and we load this first: library(tidyverse) stag &lt;- read_table2(&quot;data-raw/stag.txt&quot;) Juvenile hormone is has been set by the experimenter and mandible size has decimal places and is something we would expect to be normally distributed. Visualising our data before any further analysis is usually sensible. In this case, it will help us determine if any relationship between the two variables is linear. ggplot(data = stag, aes(x = jh, y = mand)) + geom_point() The relationship between them looks roughly linear. So far, common sense suggests the assumptions of regression are met. 1.3.2 Applying and interpreting lm() The lm() function is used to build the regression model # build the statistical model mod &lt;- lm(data = stag, mand ~ jh) This can be read as: fit a linear of model of mandible size explained by juvenile growth hormone concentration. Printing mod to the console will reveal the estimated model parameters (coefficients) but little else: mod # # Call: # lm(formula = mand ~ jh, data = stag) # # Coefficients: # (Intercept) jh # 0.419338 0.006459 \\(\\beta_{0}\\) is labelled “(Intercept)” and \\(\\beta_{1}\\) is labelled “jh”. Thus the equation of the line is: \\(mand\\) = 0.4193382 + 0.0064588\\(jh\\) More information including statistical tests of the model and its parameters is obtained by using summary() # examine it summary(mod) # # Call: # lm(formula = mand ~ jh, data = stag) # # Residuals: # Min 1Q Median 3Q Max # -0.38604 -0.20281 -0.09751 0.15034 0.60690 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.419338 0.139429 3.008 0.00941 ** # jh 0.006459 0.001584 4.078 0.00113 ** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.292 on 14 degrees of freedom # Multiple R-squared: 0.5429, Adjusted R-squared: 0.5103 # F-statistic: 16.63 on 1 and 14 DF, p-value: 0.00113 The “Coefficients:” table gives the estimated \\(\\beta_{0}\\) and \\(\\beta_{1}\\) again, this time with their standard errors and tests of whether the estimates differ from zero. The estimated value for the intercept is 0.4193382 \\(\\pm\\) 0.1394289 and this differs significantly from zero (\\(p\\) = 0.0094098). The estimated value for the slope, 0.0064588 \\(\\pm\\) 0.0015838, also differs significantly from zero (\\(p\\) = 0.0011296). The three lines at the bottom of the output gives information about the fit of the model to the data. The “Multiple R-squared” gives the proportion of the variance in the response which is explained by the model. In our case, 0.542938 of the variance in mandible length is explained by the model and this is a significant proportion of that variance (\\(p\\) = 0.0011296). For a single linear regression, the p-value for the model and the p-value for the slope are the same. This is also true for linear models in the form of a two-sample t-test but not the case for other linear models. 1.3.3 Getting predictions from the model The predict() returns the predicted values of the response. To add a column of predicted values to the dataframe: stag$pred &lt;- predict(mod) This requires creating a data frame of the x values from which you want to predict predictions &lt;- data.frame(jh = seq(0, 150, 5)) Note that the name and type of value of explanatory variable must be the same as it is in the model predictions$pred &lt;- predict(mod, newdata = predictions) Replacing the terms shown in Figure 1.1 with the values in this example gives us 1.2. Figure 1.2: these model estimates. 1.3.4 Checking assumptions plot(mod, which = 2) plot(mod, which = 1) shapiro.test(mod$res) # # Shapiro-Wilk normality test # # data: mod$res # W = 0.94737, p-value = 0.4493 1.3.5 Creating a figure ggplot(data = stag, aes(x = jh, y = mand)) + geom_point() + scale_x_continuous(expand = c(0.01, 0), limits = c(0, 160), name = expression(paste(&quot;Juvenile hormone (pg&quot;, mu, l^-1, &quot;)&quot;))) + scale_y_continuous(expand = c(0, 0), limits = c(0, 2), name = &quot;Mandible length (mm)&quot;) + geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + theme_classic() 1.3.6 Reporting the results There was a significant positive relationship between the concentration of Juvenile hormone and mandible length (\\(\\beta_{1}\\pm s.e.\\): 0.0064588 \\(\\pm\\) 0.0015838; \\(p\\) = 0.0011296). See figure 1.3. Figure 1.3: Relationship between the concentration of Juvenile hormone and mandible length. 1.4 t-tests 1.4.1 Introduction to the example Some plant biotechnologists developed a genetically modified line of Cannabis sativa to increase its omega 3 fatty acids content. They grew 50 wild type and fifty modified plants to maturity, collect the seeds and measure the amount of omega 3 fatty acids. The data are in csativa.txt. They used a two-sample t-test to compare the mean omega 3 content in the two plant types. We again use the read_table2() function to import the data and visualise it with ggplot() csativa &lt;- read_table2(&quot;data-raw/csativa.txt&quot;) # create a rough plot of the data ggplot(data = csativa, aes(x = plant, y = omega)) + geom_violin() The modified plant have a lower mean omega 3 content than the wildtype plants. The modification appears not to be successful. Statistical comparison of the two means can be done with either the t.test() or lm() functions; these are exactly equivalent but present the results differently. We will use our understanding of applying and interpreting t.test() to develop our understanding of lm() output 1.4.2 t.test() output reminder t.test(data = csativa, omega ~ plant, var.equal = TRUE) # # Two Sample t-test # # data: omega by plant # t = -5.0289, df = 98, p-value = 2.231e-06 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -9.687828 -4.205372 # sample estimates: # mean in group modif mean in group wild # 49.4652 56.4118 The two groups means are give in the section labelled “sample estimates” and the test of whether they differ significantly is given in the forth line (beginning “t = …”). We conclude the mean omega 3 content of the modified plants (49.4652) is significantly lower than that of the wildtype plants (\\(t\\) = 5.0288776, \\(d.f.\\) = 98, \\(p\\) = 2.230593910^{-6}). The confidence interval is on the difference between the two means. The sign on the \\(t\\) value and the order in which the sample estimates are given is determined by R’s alphabetical ordering of the groups. As “modif” comes before “wildtype” in the alphabet, “modif” is the first group and the test is the modified plant mean minus the wildtype mean. This has no impact on our conclusions and had the wildtype plants been labelled “control” the output would be: Two Sample t-test data: omega by plant t = 5.0289, df = 98, p-value = 2.231e-06 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 4.205372 9.687828 sample estimates: mean in group control mean in group modif 56.4118 49.4652 1.4.3 Applying and interpreting lm() The lm() function is used as follows: # build a model with `lm()` mod &lt;- lm(omega ~ plant, data = csativa) This can be read as: fit a linear of model of omega content explained by plant type. Printing mod to the console gives us these estimated model parameters (coefficients): mod # # Call: # lm(formula = omega ~ plant, data = csativa) # # Coefficients: # (Intercept) plantwild # 49.465 6.947 summary(mod) # # Call: # lm(formula = omega ~ plant, data = csativa) # # Residuals: # Min 1Q Median 3Q Max # -15.8718 -3.7026 -0.9635 4.4598 16.9182 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 49.4652 0.9768 50.642 &lt; 2e-16 *** # plantwild 6.9466 1.3813 5.029 2.23e-06 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 6.907 on 98 degrees of freedom # Multiple R-squared: 0.2051, Adjusted R-squared: 0.197 # F-statistic: 25.29 on 1 and 98 DF, p-value: 2.231e-06 anova(mod) # Analysis of Variance Table # # Response: omega # Df Sum Sq Mean Sq F value Pr(&gt;F) # plant 1 1206.4 1206.4 25.29 2.231e-06 *** # Residuals 98 4674.9 47.7 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 1.4.4 Getting predictions from the model predict(mod) # 1 2 3 4 5 6 7 8 9 10 # 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 # 11 12 13 14 15 16 17 18 19 20 # 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 # 21 22 23 24 25 26 27 28 29 30 # 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 # 31 32 33 34 35 36 37 38 39 40 # 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 # 41 42 43 44 45 46 47 48 49 50 # 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 49.4652 # 51 52 53 54 55 56 57 58 59 60 # 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 # 61 62 63 64 65 66 67 68 69 70 # 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 # 71 72 73 74 75 76 77 78 79 80 # 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 # 81 82 83 84 85 86 87 88 89 90 # 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 # 91 92 93 94 95 96 97 98 99 100 # 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 56.4118 predictions &lt;- data.frame(plant = c(&quot;modif&quot;, &quot;wild&quot;)) predictions$pred &lt;- predict(mod, newdata = predictions, interval = &quot;prediction&quot;) predictions$pred &lt;- predict(mod, newdata = predictions) Replacing the terms shown in Figure 1.1 with the values in this example gives us 1.4. Figure 1.4: these model estimates. 1.4.5 Checking assumptions plot(mod, which = 2) plot(mod, which = 1) shapiro.test(mod$res) # # Shapiro-Wilk normality test # # data: mod$res # W = 0.98816, p-value = 0.5204 1.4.6 Creating a figure csativa_summary &lt;- csativa %&gt;% group_by(plant) %&gt;% summarise(mean = mean(omega), std = sd(omega), n = length(omega), se = std/sqrt(n)) #summarise the data ggplot() + geom_jitter(data = csativa, aes(x = plant, y = omega), width = 0.2, colour = &quot;grey&quot;) + geom_errorbar(data = csativa_summary, aes(x = plant, ymin = mean, ymax = mean), width = .1) + geom_errorbar(data = csativa_summary, aes(x = plant, ymin = mean - se, ymax = mean + se), width = .2) + xlab(&quot;Plant type&quot;) + ylab(&quot;Amount of Omega 3 (units)&quot;) + ylim(0, 75) + scale_x_discrete(labels = c(&quot;Modified&quot;, &quot;Wild Type&quot;)) + theme_classic() 1.4.7 Reporting the results 1.5 One-way ANOVA 1.5.1 aov() 1.5.2 lm() 1.5.3 link between the outputs 1.5.4 post-hoc for lm() 1.5.5 reporting from lm() including a figure 1.6 Two-way ANOVA 1.6.1 aov() 1.6.2 lm() 1.6.3 link between the outputs 1.6.4 post-hoc for lm() 1.6.5 reporting from lm() including a figure 1.7 "],
["pois.html", "Chapter 2 GLM for poisson responses 2.1 intro 2.2 build 2.3 output", " Chapter 2 GLM for poisson responses 2.1 intro some stuff introducing pois 2.2 build some stuff about build pois 2.3 output some stuff about pois output "],
["bino.html", "Chapter 3 GLM for binomial responses 3.1 intro 3.2 build 3.3 output", " Chapter 3 GLM for binomial responses 3.1 intro some stuff introducing bino 3.2 build some stuff about build bino 3.3 output some stuff about bino output "],
["summary.html", "Chapter 4 Summary", " Chapter 4 Summary key points where to go next "],
["references.html", "References", " References "]
]
