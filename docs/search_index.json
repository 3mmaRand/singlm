[
["index.html", "singlm: A simple introduction to GLM for analysing Poisson and Binomial responses in R Preface 0.1 Who is this book for? 0.2 Formatting options 0.3 Conventions used in the book 0.4 Approach of this book 0.5 Outline 0.6 Following along with the examples 0.7 Overview of the chapter contents 0.8 Software information", " singlm: A simple introduction to GLM for analysing Poisson and Binomial responses in R Emma Rand 2020-08-24 Preface 0.1 Who is this book for? This book is for R users who have done an introductory class in data analysis which covered classical univariate tests such as single linear regression, t-tests, one-way ANOVA and two-way ANOVA. It is aimed at people who have a general, but not expert, understanding of summarising and graphically representing data and choosing and applying statistical tests. A revision chapter is included to set the scene and clarify terminology used in the rest of the book. I assume you have some familiarity with R and RStudio and could import data, apply t.test(), aov() and TukeyHSD() functions appropriately, interpret the results and create figures using ggplot(). I do not assume your fluency allows you to do these things with looking anything up, just that you would understand what you were doing and how to interpret the results. The book has two aims. First, to to introduce Generalised Linear Models for analysing counts and binary responses using glm(), and secondly, to introduce the terminology of statistical modelling to make your transition to more advanced texts easier. 0.2 Formatting options You can change the appearance of the book using the toolbar at the top of the page. The menu on the left can be hidden, the font size increased or decreased and the colour altered to a dark or sepia theme. Search the book by clicking on the magnifying glass, entering a search term and using the up and down arrows to navigate through the results. 0.3 Conventions used in the book Code and any output appears in blocks formatted like this: stag &lt;- read_table2(&quot;data-raw/stag.txt&quot;) glimpse(stag) # Rows: 16 # Columns: 2 # $ jh &lt;dbl&gt; 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140... # $ mand &lt;dbl&gt; 0.56, 0.35, 0.28, 1.22, 0.48, 0.86, 0.68, 0.77, 0.55, 1.18, 0.... Lines of output start with a #. The content of a code block can be copied using the icon in its top right. Within the text: - packages are indicated in bold code font like this: ggplot2 - functions are indicated in code font with brackets after their name like this: ggplot() - R objects are indicated in code font like this: stag Key points are summarised throughout the book using boxes like this: The key point of a previous few paragraphs is in boxes like these Extra pieces of information that are not essential to understanding the material are presented like this: Extra information and tips are in boxes like these I use packages from the tidyverse (Wickham et al. 2019) throughout and chapters assume it has been loaded. If you get an error like this: # Error in read_table2(&quot;data-raw/stag.txt&quot;) : # could not find function &quot;read_table2&quot; Then load the tidyverse like this: library(tidyverse) All other packages will be loaded explicitly where needed with library() statements. 0.4 Approach of this book Regression, t-tests and one-way ANOVA are fundamentally the same test and can all be carried out with the lm() function in R. However, it is common for t-tests and ANOVA to be taught using the t.test() and aov() functions respectively. One reason for this is because their outputs are easier for beginners to understand and interpret. However, the output of lm() is more typical of statistical modelling functions in general and not using lm() for the relatively simple cases makes it more difficult for people to extend their the statistical repertoire. The approach taken in this book is to exploit preexisting knowledge of t-tests and ANOVA using t.test() and aov() to understand the output of lm(). This will make it easier to understand the output of glm(). 0.5 Outline This book introduces the the Generalised Linear Model for two types of discrete response: Binomially distributed: when a response variable can take one of only two values, such as “yes” or “no”, “alive” or “dead”, “present” or “absent”. Poisson distributed: when a response variable is the number of things. In R, these are analysed with the glm() function. lm() can be used to perform t-tests, ANOVAs and regression. glm() can be used to perform tests using the Generalised Linear Model for response variables which are counts or binary. Models are explained with reference to examples. Each example demonstrates the R code needed, how understand the output and how to report the results, including suggested ggplot2figures. The code is given for figures but not extensively explained. To learn more go to https://ggplot2.tidyverse.org/ 0.6 Following along with the examples instructions rstudio projects, data-raw, script file, down load “data-raw/stag.txt” 0.7 Overview of the chapter contents Part 1 provides a brief revision of an introductory data analysis class using terminology that will used throughout the remaining chapters. Single linear regression is used as an example of lm(). If the concepts in this chapter are very unfamiliar, you may benefit from revising your previous work. In Part 2 works through t-test, one-way ANOVA and two-way ANOVA examples carried out first with t.test() and aov() and then with lm() to gain a good understanding of the lm() output and interrogation for reporting. Part 3 gives a introduction to generalised linear models. Part 4 gives a introduction to generalised linear models for count data and works through several examples. Part 5 gives a introduction to generalised linear models for binomial data and works through several examples. 0.8 Software information I used the knitr package (Xie 2015) and the bookdown package (Xie 2020) to compile my book. My R session information is shown below: sessionInfo() # R version 4.0.2 (2020-06-22) # Platform: x86_64-w64-mingw32/x64 (64-bit) # Running under: Windows 10 x64 (build 16299) # # Matrix products: default # # locale: # [1] LC_COLLATE=English_United Kingdom.1252 # [2] LC_CTYPE=English_United Kingdom.1252 # [3] LC_MONETARY=English_United Kingdom.1252 # [4] LC_NUMERIC=C # [5] LC_TIME=English_United Kingdom.1252 # # attached base packages: # [1] stats graphics grDevices utils datasets methods base # # other attached packages: # [1] patchwork_1.0.1 kableExtra_1.1.0 forcats_0.5.0 stringr_1.4.0 # [5] dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.1 # [9] tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 # # loaded via a namespace (and not attached): # [1] tidyselect_1.1.0 xfun_0.16 haven_2.3.1 colorspace_1.4-1 # [5] vctrs_0.3.2 generics_0.0.2 htmltools_0.5.0 viridisLite_0.3.0 # [9] yaml_2.2.1 utf8_1.1.4 blob_1.2.1 rlang_0.4.7 # [13] pillar_1.4.6 glue_1.4.1 withr_2.2.0 DBI_1.1.0 # [17] dbplyr_1.4.4 modelr_0.1.8 readxl_1.3.1 lifecycle_0.2.0 # [21] munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 rvest_0.3.6 # [25] evaluate_0.14 knitr_1.29 fansi_0.4.1 broom_0.7.0 # [29] Rcpp_1.0.5 scales_1.1.1 backports_1.1.7 webshot_0.5.2 # [33] jsonlite_1.7.0 fs_1.5.0 hms_0.5.3 digest_0.6.25 # [37] stringi_1.4.6 bookdown_0.20 grid_4.0.2 cli_2.0.2 # [41] tools_4.0.2 magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 # [45] ellipsis_0.3.1 xml2_1.3.2 reprex_0.3.0 lubridate_1.7.9 # [49] assertthat_0.2.1 rmarkdown_2.3 httr_1.4.2 rstudioapi_0.11 # [53] R6_2.4.1 compiler_4.0.2 References "],
["revision.html", "Chapter 1 Revision of your Introductory class", " Chapter 1 Revision of your Introductory class In experimental design and execution we manipulate, or choose, one or more variables and record the effect of this manipulation on another variable. The variables we manipulate are called explanatory or predictor variables and the other is called the response. These are also known as independent and dependent variables respectively. Predictor, Explanatory, x and Independent: all terms used to describe the variables we choose. Predicted, Response, y and Dependent: all terms used to describe the variable we measure. When we plot data, the response variable goes on the y-axis and the explanatory variable goes on the x-axis. If we have two explanatory variables we might indicate the different values of one of them with colour. See Figure 1.1. Figure 1.1: Explanatory variables are placed on the x-axis and, if there is more than one, indicated with different colours (or shapes) and panels. The response variable is always on the y-axis. In choosing between regression, t-tests, one-way ANOVA and two-way ANOVA we consider how many explanatory variables we have and whether they are continuous or categorical. If we have one explanatory variable and it is continuous, we can apply a regression. If it is a categorical variable with two groups (or levels) we have the choice of a t-test or a one way ANOVA but when there are more than two groups we use a one-way ANOVA. A two-way ANOVA is used when there are two categorical explanatory variables. See Figure 1.2. Figure 1.2: Decision tree for choosing between single linear regression, t-tests, one-way ANOVA and two-way ANOVA. These apparently different tests are, in fact, the same test. They have the same underlying mathematics, or to put it another way, the follow the same model. That model is often known as the General Linear Model. "],
["what-are-linear-models.html", "Chapter 2 What are General Linear Models 2.1 Overview 2.2 Model fitting 2.3 More than one explanatory variable 2.4 General linear models in R 2.5 Reporting", " Chapter 2 What are General Linear Models 2.1 Overview A linear model describes a continuous response variable as a function of one or more explanatory variables. When you have a single explanatory variable, that model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i} \\tag{2.1} \\end{equation}\\] Where: \\(y\\) is the response variable and \\(X1\\) is the explanatory variable. \\(i\\) is the index so \\(X1_{i}\\) is the \\(i\\)th value of \\(X1\\) \\(E(y_{i})\\) is the expected value of \\(y\\) for the \\(i\\)th value of \\(X1\\). \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the coefficients in the model. In a single linear regression, \\(\\beta_{0}\\) is often called the intercept and \\(\\beta_{1}\\) the slope. The equation (2.1) allows the response, \\(y\\) to be predicted for a given value of the explanatory variable. Let’s unpack what we mean by \\(E(y_{i})\\). If you measure the response for a particular value of \\(x\\) very many times there would be some distribution of those responses. In the general linear model that distribution is assumed to be normal. Its mean is \\(E(y_{i})\\). Another way of saying that is that in a general linear model, we model the mean of the response. That the measured response is drawn from normal distribution with a mean of \\(E(y_{i})\\) is a defining feature of the general linear model. An additional assumption is that those normal distributions have the same variance for all the \\(x\\) values 2.2 Model fitting The process of estimating the model coefficients from your data (set of chosen \\(X1\\) with their measured \\(y\\) values) is known as fitting a linear model. The coefficients are also known as parameters. The measured response values in your data, \\(y_{i}\\), will differ from the predicted values, \\(\\hat{y}\\), randomly and these random differences are known as residuals or errors. Our parameter values are chosen to minimise the sum of the squared residuals and is known as least-squares estimation. A very commonly used abbreviation for the sum of the squared residuals (or errors) is \\(SSE\\). \\[\\begin{equation} SSE = \\sum(y_{i}-\\hat{y})^2 \\tag{2.2} \\end{equation}\\] The role played by \\(SSE\\) in estimating our parameters means that it is also used in determining how well our model fits our data. Our model can be considered useful if its predictions are close to the observed data and the smaller the value of \\(SSE\\), the better the fit. In other words, there is little random variance left over in the response. As the absolute value of \\(SSE\\) will depend on the size of the \\(y\\) values and the sample size, we express it as a proportion or the total variation in \\(y\\), \\(SST\\) and proportion of variance left over is \\[\\begin{equation} SSE / SST \\tag{2.3} \\end{equation}\\] The proportion of variance explained by the model is a very commonly used metric of model fit and you have probably heard of it - \\(R^2\\) \\[\\begin{equation} R^2=1-\\frac{SSE}{SST} \\tag{2.3} \\end{equation}\\] If there were no explanatory variables, the value we would predict for the response variable is its mean. Thus a good model should fit the response better than the mean. The output of lm() includes a measure of fit called R-squared which is the proportion of variance explained by the model relative to the total variance of the response. This is the proportional improvement in the predictions from the regression model relative to the mean model. It ranges from zero, the model is no better than the mean, to 1, the predictions are perfect. See Figure 2.1. Figure 2.1: A linear model with different fits. A) the model is a poor fit - the explanatory variable is no better than the response mean for predicting the response. B) the model is good fit - the explanatory variable explains a high proportion of the variance in the response. C) the model is a perfect fit - the response can be predicted perfectly from the explanatory variable. Measured response values are in pink and the predictions are in green. Adjusted \\(R^\\) penalises…. Since the distribution of the responses for a given \\(x\\) is assumed to be normal and variances of those distributions are assumed to be homogeneous, these are also true of the residuals and it is our examination of the residuals which allows us to evaluate whether the assumptions are met. See Figure 2.2 for a graphical representation of linear modelling terms introduced so far. We will reference this figure in later chapters. Figure 2.2: A general linear model annotated with the terms used in modelling. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) (the intercept) and \\(\\beta_{1}\\) (the slope) are indicated. 2.3 More than one explanatory variable When you have more than one explanatory variable these are given as \\(X2\\), \\(X3\\) and so on up to the \\(p\\)th explanatory variable. Each explanatory variable has its own \\(\\beta\\) coefficient. The general form of the model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+...+\\beta_{p}Xp_{i} \\tag{2.4} \\end{equation}\\] The model has only one intercept which is the value of the response when all the explanatory variables are zero. 2.4 General linear models in R 2.4.1 Building and viewing T-tests and ANOVA, like regression, can be carried out with the lm() function in R. It uses the same method for specifying the model. When you have one explanatory variable the command is: lm(data = dataframe, response ~ explanatory) The response ~ explanatory part is known as the model formula. When you have two explanatory variable we add the second explanatory variable to the formula using a + or a *. The command is: lm(data = dataframe, response ~ explanatory1 + explanatory2) or lm(data = dataframe, response ~ explanatory1 * explanatory2) A model with explanatory1 + explanatory2 considers the effects of the two variables independently. A model with explanatory1 + explanatory2 considers the effects of the two variables and any interaction between them. We usually assign the output of lm() commands to an object and view it with summary(). The typical workflow would be: mod &lt;- lm(data = dataframe, response ~ explanatory) summary(mod) There are two sorts of statistical tests in the output of summary(mod): tests of whether each coefficient is significantly different from zero; and an F-test of the model overall. The F-test in the last line of the output indicates whether the relationship modelled between the response and the set of explanatory variables is statistically significant. lm() can be used to perform tests using the General Linear Model including t-tests, ANOVA and regression for response variables which are normally distributed. Elements of the lm() object include the estimated coefficients, the predicted values and the residuals can be accessed with mod$coeffients, mod$fitted.values and mod$residuals respectively. 2.4.2 Getting predictions mod$fitted.values gives the predicted values for the explanatory variable values actually used in the experiment, i.e., there is a prediction for each row of data. To get predictions for a different set of values make a dataframe of the different set of values and use the predict() function.. The typical workflow would be: predict_for &lt;- data.frame(explanatory = values) predict_for$pred &lt;- predict(mod, newdata = predict_for) 2.4.3 Checking assumptions The assumptions of the model are checked using the plot() function which produces diagnostic plots to explore the distribution of the residuals. They are not proof of the assumptions being met but allow us to quickly determine if the assumptions are plausible, and if not, how the assumptions are violated and what data points contribute to the violation. The two plots which are most useful are the “Q-Q” plot (plot 2) and the “Residuals vs Fitted” plot (plot 1). These are given as values to the which argument of plot(). The Q-Q plot is a scatterplot of the residuals (standardised to a mean of zero and a standard deviation of 1) against what is expected if the residuals are normally distributed. plot(mod, which = 2) The points should fall roughly on the line if the residuals are normally distributed. The following are two examples in which the residuals are not normally distributed. The Residuals vs Fitted plot shows if residuals have homogeneous variance or have non-linear patterns. Non-linear relationship between explanatory variables and the response will usually show in this plot if the model does not capture the non-linear relationship. For the assumptions to be met, the residuals should be equally spread around a horizontal line: plot(mod, which = 1) The following are two examples in which the residuals do not have homogeneous variance and display non-linear patterns. 2.5 Reporting to add figure: data + model summarise but in a way that is ‘honest’ direction and magnitude of effects, significance, stats result. "],
["terminolgy.html", "Chapter 3 Terminolgy 3.1 General? Generalised? 3.2 Testing modelling fitting 3.3 paramters, coefficients estimates", " Chapter 3 Terminolgy 3.1 General? Generalised? Before we get started, I need to explain some potentially confusing terminology. I use the term “general linear model” to mean classical linear regression models of a continuous response variable explained by one or more continuous or categorical variables. These include regression, t-tests, analysis of variance (ANOVA) and analysis of covariance (ANCOVA). I use the term “generalised linear model” or GLM to refers to a larger class of models formulated by John Nelder and Robert Wedderburn (1972) and popularised in “Generalized Linear Models”, an influential book by Peter McCullagh and John Nelder (1989). In these models, the response variable can follow a number of other distributions including Poisson and binomial distributions. In some texts, the general linear model is referred to by the acronym GLM and the generalised linear model as GLIM but in this book I use GLM to refer to generalised linear models only. 3.2 Testing modelling fitting 3.3 paramters, coefficients estimates References "],
["single-regression.html", "Chapter 4 Single linear regression 4.1 Introduction to the example 4.2 Applying and interpreting lm() 4.3 Link to Chapter 2.1 4.4 Getting predictions from the model 4.5 Checking assumptions 4.6 Creating a figure 4.7 Reporting the results", " Chapter 4 Single linear regression This is probably the one general linear model you have applied out using lm() previously and it is covered here as revision and make more clear links between regression, t-tests and ANOVA. 4.1 Introduction to the example Figure 4.1: Male stag beetles Lucanus cervus, have large mandibles that resemble the antlers of a stag and give them their common and scientific name (Cervus is a genus of deer). By Simon A. Eugster - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=7790887 The concentration of Juvenile growth hormone in male stag beetles (Lucanus cervus) is known to influence mandible growth. See Figure 4.1 Groups of ten stag beetles were treated with different concentrations of Juvenile growth hormone (pg\\(\\mu\\)l-1) and their average mandible size (mm) determined. The data are in stag.txt. Juvenile hormone is has been set by the experimenter and we would expect mandible size to be normally distributed. jh mand 0 0.56 10 0.35 20 0.28 30 1.22 40 0.48 50 0.86 60 0.68 70 0.77 80 0.55 90 1.18 100 0.71 110 1.44 120 1.32 130 1.66 140 1.23 150 1.17 There are 2 variables: jh, the concentration of Juvenile growth hormone and mand, the average mandible size (mm) of 10 stag beetles We will import the data with the read_table2() function from the readr package and plot it with ggplot() from the ggplot2 package. Both packages are part of the tidyverse. Import the data: stag &lt;- read_table2(&quot;data-raw/stag.txt&quot;) Visualising our data before any further analysis is sensible. In this case, it will help us determine if any relationship between the two variables is linear. A simple scatter plot is appropriate. ggplot(data = stag, aes(x = jh, y = mand)) + geom_point() The relationship between the two variables looks roughly linear. So far, common sense suggests the assumptions of regression are met. 4.2 Applying and interpreting lm() The lm() function is used to build the regression model: mod &lt;- lm(data = stag, mand ~ jh) This can be read as: fit a linear of model of mandible size explained by the concentration of Juvenile growth hormone. Printing mod to the console will reveal the estimated model parameters (coefficients) but little else: mod # # Call: # lm(formula = mand ~ jh, data = stag) # # Coefficients: # (Intercept) jh # 0.41934 0.00646 \\(\\beta_{0}\\) is labelled (Intercept) and \\(\\beta_{1}\\) is labelled jh. Thus, the equation of the line is: \\(mand\\) = 0.419 + 0.006\\(jh\\) More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # lm(formula = mand ~ jh, data = stag) # # Residuals: # Min 1Q Median 3Q Max # -0.3860 -0.2028 -0.0975 0.1503 0.6069 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.41934 0.13943 3.01 0.0094 ** # jh 0.00646 0.00158 4.08 0.0011 ** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.292 on 14 degrees of freedom # Multiple R-squared: 0.543, Adjusted R-squared: 0.51 # F-statistic: 16.6 on 1 and 14 DF, p-value: 0.00113 The Coefficients table gives the estimated \\(\\beta_{0}\\) and \\(\\beta_{1}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated value for the intercept is 0.419 \\(\\pm\\) 0.139 and this differs significantly from zero (\\(p\\) = 0.009). The estimated value for the slope is 0.006 \\(\\pm\\) 0.002, also differs significantly from zero (\\(p\\) = 0.001). The three lines at the bottom of the output give information about the fit of the model to the data. The Multiple R-squared gives the proportion of the variance in the response which is explained by the model. In our case, 0.543 of the variance in mandible length is explained by the model and this is a significant proportion of that variance (\\(p\\) = 0.001). The p-value for the model and the p-value for the slope are the same in a single linear regression because, except for the intercept, there is only one parameter (the slope) in the model. Linear models in the form of a two-sample t-test also estimate just one parameter and its p-value will also equal the model p-value. This is not the case for other linear models. 4.3 Link to Chapter 2.1 The estimated coefficients tell us mandible size is predicted to be 0.419 when Juvenile growth hormone is zero and increases by 0.006 mm for each pg\\(\\mu\\)l-1 of Juvenile growth hormone. At a Juvenile growth hormone of 1 pg\\(\\mu\\)l-1 the mandible is predicted to be 0.419 + 0.006 = 0.426 mm. At 2 pg\\(\\mu\\)l-1 the predicted mandible size is 0.419 + 0.006 + 0.006 = 0.432 mm. In general mandible size is: \\(\\beta_{0}\\) + \\(x\\times\\beta_{0}\\) mm at \\(x\\) pg\\(\\mu\\)l-1. See Figure 4.2 for a version of Figure 2.2 annotated with values from this example. Figure 4.2: The model annotated with values from the stag beetle example. The measured response values are in pink, the predictions are in green, and the residuals, are in blue. One example of a measured value, a predicted value and the residual is shown for a Juvenile hormone of 130 pg\\(\\mu\\)l-1. The estimated model parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are indicated. Compare to Figure 2.2. 4.4 Getting predictions from the model The predict() returns the predicted values of the response. To add a column of predicted values to the stag dataframe we use: stag$pred &lt;- predict(mod) glimpse(stag) # Rows: 16 # Columns: 3 # $ jh &lt;dbl&gt; 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140... # $ mand &lt;dbl&gt; 0.56, 0.35, 0.28, 1.22, 0.48, 0.86, 0.68, 0.77, 0.55, 1.18, 0.... # $ pred &lt;dbl&gt; 0.419, 0.484, 0.549, 0.613, 0.678, 0.742, 0.807, 0.871, 0.936,... This gives predictions for the actual Juvenile growth hormone concentration values used. If you want predictions for other values, you need to create a data frame of the Juvenile growth hormone values from which you want to predict. The following creates a dataframe with one column of Juvenile growth hormone values from 0 to 150 in steps of 5: predict_for &lt;- data.frame(jh = seq(0, 150, 5)) glimpse(predict_for) # Rows: 31 # Columns: 1 # $ jh &lt;dbl&gt; 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80... Note that the column is named jh - the same as in the dataset and the model. Its variable type must also match. To predict responses for a new set of explanatory variable values, the name and type of explanatory variables in the new set must match those in the model. To get predicted mandible sizes for the Juvenile growth hormone values we use: predict_for$pred &lt;- predict(mod, newdata = predict_for) glimpse(predict_for) # Rows: 31 # Columns: 2 # $ jh &lt;dbl&gt; 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, ... # $ pred &lt;dbl&gt; 0.419, 0.452, 0.484, 0.516, 0.549, 0.581, 0.613, 0.645, 0.678,... 4.5 Checking assumptions The two assumptions of the model can be checked using diagnostic plots. The Q-Q plot is obtained with: plot(mod, which = 2) This sample is relatively small so we should expect more wiggliness than we saw in 2.2 but this looks OK. Let’s look at the Residuals vs Fitted plot: plot(mod, which = 1) Again the red line wiggles a little but there is no particular pattern and it appears that the variance is homogeneous along mandible size. 4.6 Creating a figure A suitable figure includes the data themselves and the model fitted: ggplot(data = stag, aes(x = jh, y = mand)) + geom_point() + scale_x_continuous(expand = c(0.01, 0), limits = c(0, 160), name = expression(paste(&quot;Juvenile growth hormone (pg&quot;, mu, l^-1, &quot;)&quot;))) + scale_y_continuous(expand = c(0, 0), limits = c(0, 2), name = &quot;Mandible length (mm)&quot;) + geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + theme_classic() 4.7 Reporting the results to add: principle, sig, magnitude and diection of effects, test result, figure There was a significant positive relationship between the concentration of Juvenile growth hormone and mandible length (\\(\\beta_{1}\\pm s.e.\\): 0.006 \\(\\pm\\) 0.002; \\(p\\) = 0.001). See figure 4.3. Figure 4.3: Relationship between the concentration of Juvenile growth hormone and mandible length. "],
["t-tests-revisit.html", "Chapter 5 t-tests revisited 5.1 Introduction to the example 5.2 t.test() output reminder 5.3 t-tests as linear models 5.4 Applying and interpreting lm() 5.5 Getting predictions from the model 5.6 Checking assumptions 5.7 Creating a figure 5.8 Reporting the results", " Chapter 5 t-tests revisited In this chapter we look at an example with one categorical explanatory variable which has two groups (or levels). We first use the familiar t.test() then use its output to help us understand the output of lm(). We will also make predictions from the model and report on our results. 5.1 Introduction to the example Some plant biotechnologists developed a genetically modified line of Cannabis sativa to increase its omega 3 fatty acids content. They grew 50 wild type and fifty modified plants to maturity, collect the seeds and measure the amount of omega 3 fatty acids (in arbitrary units). The data are in csativa.txt. They want to know if the wild type and modified plants differ significantly in their omega 3 fatty acid content. omega plant 48.5 modif 43.6 modif 51.2 modif 56.4 modif 56.0 modif 58.7 modif 39.1 modif 48.8 modif 55.5 modif 44.6 modif 46.5 modif 41.5 modif 40.8 modif 45.1 modif 46.1 modif 39.4 modif 47.2 modif 48.0 modif 50.7 modif 48.2 modif 48.4 modif 48.1 modif 56.7 modif 49.6 modif 49.1 modif 47.4 modif 59.9 modif 54.3 modif 61.9 modif 46.7 modif 58.3 modif 41.9 modif 52.7 modif 54.5 modif 59.6 modif 49.6 modif 47.4 modif 53.4 modif 48.1 modif 53.8 modif 42.8 modif 45.8 modif 42.4 modif 48.2 modif 49.8 modif 50.1 modif 48.4 modif 61.0 modif 41.3 modif 46.3 modif 58.5 wild 55.5 wild 58.7 wild 67.7 wild 41.4 wild 48.0 wild 64.5 wild 52.2 wild 54.2 wild 40.5 wild 59.1 wild 68.5 wild 47.9 wild 60.5 wild 63.0 wild 57.5 wild 58.0 wild 70.2 wild 67.1 wild 52.7 wild 60.2 wild 42.5 wild 60.2 wild 53.8 wild 45.4 wild 53.2 wild 63.3 wild 45.3 wild 65.3 wild 61.9 wild 49.2 wild 73.3 wild 70.3 wild 56.3 wild 56.0 wild 53.5 wild 63.6 wild 45.9 wild 54.5 wild 54.6 wild 50.9 wild 58.2 wild 54.5 wild 56.6 wild 54.1 wild 53.5 wild 56.9 wild 46.0 wild 50.0 wild 65.7 wild There are 2 variables. plant is the explanatory variable; it is categorical with 2 levels, modif and wild. omega, a continuous variable, is the response. We again use the read_table2() function to import the data and visualise it with ggplot() csativa &lt;- read_table2(&quot;data-raw/csativa.txt&quot;) A quick plot of the data: ggplot(data = csativa, aes(x = plant, y = omega)) + geom_violin() Violin plots are a useful way to show the distribution of data in each group but not the only way. One alternative is geom_boxplot(). The modified plant have a lower mean omega 3 content than the wild type plants. The modification appears not to be successful. In fact, it may have significantly lowered the omega 3 content! Statistical comparison of the two means can be done with either the t.test() or lm() functions; these are exactly equivalent but present the results differently. We will use our understanding of applying and interpreting t.test() to develop our understanding of lm() output 5.2 t.test() output reminder We can apply a two-sample t-test with: t.test(data = csativa, omega ~ plant, var.equal = TRUE) # # Two Sample t-test # # data: omega by plant # t = -5, df = 98, p-value = 2e-06 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -9.69 -4.21 # sample estimates: # mean in group modif mean in group wild # 49.5 56.4 The two groups means are given in the section labelled sample estimates and the test of whether they differ significantly is given in the fourth line (beginning t = -5, df = ...). We conclude the mean omega 3 content of the modified plants (49.465 units) is significantly lower than that of the wild type plants (\\(t\\) = 5.029, \\(d.f.\\) = 98, \\(p\\) &lt; 0.001). The line under 95 percent confidence intervalgives the confidence limits on the difference between the two means. The sign on the \\(t\\) value and the confidence limits, and the order in which the sample estimates are given is determined by R’s alphabetical ordering of the groups. As “modif” comes before “wild” in the alphabet, “modif” is the first group and the test is the modified plant mean minus the wild type mean. This has no impact on our conclusions. If the wild type plants been labelled “control” so that “modif” would be the second group, our output would look like this: # Two Sample t-test # # data: omega by plant # t = 5.0289, df = 98, p-value = 2.231e-06 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 4.205372 9.687828 # sample estimates: # mean in group control mean in group modif # 56.4118 49.4652 # t.test() output: the estimates are the two group means and the p-value is for a test on the difference between them. 5.3 t-tests as linear models The equation for a t-test is just as it was for equation (2.1): \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i} \\tag{5.1} \\end{equation}\\] Remember, in a single linear regression \\(\\beta_{0}\\), the intercept, is the value of the response when the numerical explanatory variable is zero. So what does this mean when the explanatory variable is categorical? It means the intercept is the value of the response when the categorical explanatory is at its “lowest” level where the “lowest” level is the group which comes first alphabetically. \\(X1_{i}\\) is an indicator variable that takes the value of 0 or 1 and indicates whether the \\(i\\)th value was from one group or not. Such variables are known as dummy explanatory variables. They are dummy in the sense that they are numerical substitutes for the categorical variable whose ‘real’ values are the names of the categories. You can think of \\(X1_{i}\\) as toggling on and off the \\(\\beta_{1}\\) effect: If it has a value of 0 for a data point it means that \\(\\beta_{1}\\) will not impact the response which will be \\(\\beta_{0}\\). If it has a value 1 then \\(\\beta_{1}\\) will change the response to \\(\\beta_{0}\\) + \\(\\beta_{1}\\) \\(\\beta_{1}\\) is thus the difference between the group means. A graphical representation of the terms in a linear model when the explanatory variable is categorical with two groups is given in Figure 5.1. Figure 5.1: A linear model when the explanatory variable is categorical with two groups annotated with the terms used in linear modelling. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters are indicated: \\(\\beta_{0}\\) is the mean of group A and \\(\\beta_{1}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group B. Compare to Figure 2.2. 5.4 Applying and interpreting lm() The lm() function is applied to this example as follows: mod &lt;- lm(data = csativa, omega ~ plant) This can be read as: fit a linear of model of omega content explained by plant type. Notice that the model formula is the same in both the t.test() and the lm() functions. Printing mod to the console gives us the estimated model parameters (coefficients): mod # # Call: # lm(formula = omega ~ plant, data = csativa) # # Coefficients: # (Intercept) plantwild # 49.47 6.95 The first group of plant is modif so \\(\\beta_{0}\\) is the mean of the modified plants. \\(\\beta_{1}\\) is the coefficient labelled plantwild. In R, the coefficients are consistently named like this: variable name followed by the value without spaces. It means when the variable plant takes the value wild, \\(\\beta_{1}\\) must be added to \\(\\beta_{0}\\) Thus, the mean omega 3 in the modified plants is 49.465 units and that in the wild type plants is 49.465 + 6.947 = 56.412 units. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # lm(formula = omega ~ plant, data = csativa) # # Residuals: # Min 1Q Median 3Q Max # -15.872 -3.703 -0.964 4.460 16.918 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 49.465 0.977 50.64 &lt; 2e-16 *** # plantwild 6.947 1.381 5.03 2.2e-06 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 6.91 on 98 degrees of freedom # Multiple R-squared: 0.205, Adjusted R-squared: 0.197 # F-statistic: 25.3 on 1 and 98 DF, p-value: 2.23e-06 The Coefficients table gives the estimated \\(\\beta_{0}\\) and \\(\\beta_{1}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated mean of the modified plants is 49.465 \\(\\pm\\) 0.977 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated difference between the modified and wild type plants is 6.947 \\(\\pm\\) 1.381 and also differs significantly from zero (\\(p\\) &lt; 0.001). The fact that this value is positive tells us that the wild type plants have a higher mean. The proportion of the variance in the omega which is explained by the model is 0.205 and this is a significant proportion of that variance (\\(p\\) &lt; 0.001). As was true for single linear regression, the p-value for the model and the p-value for the difference between the means are the same because there is only one parameter in the model after the intercept. Replacing the terms shown in Figure 5.1 with the values in this example gives us 5.2. Figure 5.2: The annotated model with the values from the Omega 3 content of Cannabis sativa example. The measured response values are in pink, the predictions are in green, and the residuals, are in blue. One example of a measured value, a predicted value and the residual is shown for a wild type individual. The estimated model parameters are indicated: \\(\\beta_{0}\\), the mean of the modified plants, is 49.465 and \\(\\beta_{1}\\) is 6.947. Thus the mean of wildtype plants is 49.465 + 6.947 = 56.412 units. Compare to Figure 5.1. 5.5 Getting predictions from the model We already have the predictions for all possible values of the explanatory variable because there are only two! However the code for using predict is included here because it will make it easier to understand more complex examples later. We need to create a dataframe of values for which we want predictions and pass it as an argument to the predict() function. To create a dataframe with one column of Plant values: predict_for &lt;- data.frame(plant = c(&quot;modif&quot;, &quot;wild&quot;)) Remember! The variable and its values have to exactly match those in the model. The to get the predicted omega content for the two plant types: predict_for$pred &lt;- predict(mod, newdata = predict_for) glimpse(predict_for) # Rows: 2 # Columns: 2 # $ plant &lt;chr&gt; &quot;modif&quot;, &quot;wild&quot; # $ pred &lt;dbl&gt; 49.5, 56.4 5.6 Checking assumptions The two assumptions of the model can be checked using diagnostic plots. The Q-Q plot is obtained with: plot(mod, which = 2) The residual seem to be normally distributed. Let’s look at the Residuals vs Fitted plot: plot(mod, which = 1) We get these two columns of points because the explanatory variable, plant, is categorical so the fitted - or predicted - values are just two means. In my view, the variance looks higher in the group with the higher mean (on the right). 5.7 Creating a figure csativa_summary &lt;- csativa %&gt;% group_by(plant) %&gt;% summarise(mean = mean(omega), std = sd(omega), n = length(omega), se = std/sqrt(n)) #summarise the data ggplot() + geom_jitter(data = csativa, aes(x = plant, y = omega), width = 0.25, colour = &quot;grey&quot;) + geom_errorbar(data = csativa_summary, aes(x = plant, ymin = mean, ymax = mean), width = .3) + geom_errorbar(data = csativa_summary, aes(x = plant, ymin = mean - se, ymax = mean + se), width = .5) + geom_segment(aes(x = 1, y = 75, xend = 2, yend = 75), size = 1) + geom_segment(aes(x = 1, y = 75, xend = 1, yend = 73), size = 1) + geom_segment(aes(x = 2, y = 75, xend = 2, yend = 73), size = 1) + annotate(&quot;text&quot;, x = 1.5, y = 77, label = &quot;***&quot;, size = 6) + scale_x_discrete(labels = c(&quot;Modified&quot;, &quot;Wild Type&quot;), name = &quot;Plant type&quot;) + scale_y_continuous(name = &quot;Amount of Omega 3 (units)&quot;, expand = c(0, 0), limits = c(0, 90)) + theme_classic() 5.8 Reporting the results to add: principle, sig, magnitude and diection of effects, test result, figure The genetic modification was unsuccessful with wild type plants (\\(\\bar{x} \\pm s.e.\\): 56.412 \\(\\pm\\) 1.11 units) have significantly higher omega 3 than modified plants(49.465 \\(\\pm\\) 0.823 units) (\\(t\\) = 5.029; \\(d.f.\\) = 98; \\(p\\) &lt; 0.001). See figure 5.3. Figure 5.3: Relationship between the concentration of Juvenile growth hormone and mandible length. "],
["one-way-anova-revisit.html", "Chapter 6 One-way ANOVA revisited 6.1 Introduction to the example 6.2 aov() output reminder 6.3 Post-hoc testing for aov() 6.4 One-way ANOVA as linear models 6.5 Applying and interpreting lm() 6.6 Getting predictions from the model 6.7 Checking assumptions 6.8 Post-hoc testing for lm() 6.9 Creating a figure 6.10 Reporting the results", " Chapter 6 One-way ANOVA revisited In this chapter we again consider an example with one categorical explanatory variable. However, this time it has more than two groups (or levels). We first use the familiar aov() function to carry out a one-way ANOVA and then use our understanding to help us understand the output of lm(). We will also make predictions from the model and report on our results. 6.1 Introduction to the example Figure 6.1: Baby Weddell Seals are very cute. By Photo © Samuel Blanc, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=3877642 The myoglobin concentration of skeletal muscle (in grams per kilogram of muscle) for three species of seal (see Figure 6.1) is given seal.txt. species myoglobin Harbour Seal 49.7 Harbour Seal 51.0 Harbour Seal 41.6 Harbour Seal 45.6 Harbour Seal 39.4 Harbour Seal 43.1 Harbour Seal 55.7 Harbour Seal 66.1 Harbour Seal 56.0 Harbour Seal 47.0 Harbour Seal 51.5 Harbour Seal 50.9 Harbour Seal 53.4 Harbour Seal 35.3 Harbour Seal 45.2 Harbour Seal 38.4 Harbour Seal 50.5 Harbour Seal 49.4 Harbour Seal 41.6 Harbour Seal 41.5 Harbour Seal 45.8 Harbour Seal 48.1 Harbour Seal 57.3 Harbour Seal 61.2 Harbour Seal 48.8 Harbour Seal 62.2 Harbour Seal 38.0 Harbour Seal 40.6 Harbour Seal 67.3 Harbour Seal 48.5 Weddell Seal 55.4 Weddell Seal 40.1 Weddell Seal 46.3 Weddell Seal 29.8 Weddell Seal 52.5 Weddell Seal 37.4 Weddell Seal 42.8 Weddell Seal 51.4 Weddell Seal 48.5 Weddell Seal 44.0 Weddell Seal 58.0 Weddell Seal 45.4 Weddell Seal 37.1 Weddell Seal 39.3 Weddell Seal 45.1 Weddell Seal 51.1 Weddell Seal 38.1 Weddell Seal 36.5 Weddell Seal 49.4 Weddell Seal 62.0 Weddell Seal 45.7 Weddell Seal 57.0 Weddell Seal 42.6 Weddell Seal 40.0 Weddell Seal 31.9 Weddell Seal 42.7 Weddell Seal 46.0 Weddell Seal 39.0 Weddell Seal 50.1 Weddell Seal 34.4 Bladdernose Seal 56.2 Bladdernose Seal 48.4 Bladdernose Seal 37.8 Bladdernose Seal 42.8 Bladdernose Seal 27.0 Bladdernose Seal 43.1 Bladdernose Seal 42.4 Bladdernose Seal 29.9 Bladdernose Seal 42.3 Bladdernose Seal 58.1 Bladdernose Seal 32.2 Bladdernose Seal 38.4 Bladdernose Seal 52.6 Bladdernose Seal 53.9 Bladdernose Seal 42.3 Bladdernose Seal 46.4 Bladdernose Seal 44.6 Bladdernose Seal 49.0 Bladdernose Seal 40.2 Bladdernose Seal 41.4 Bladdernose Seal 38.6 Bladdernose Seal 35.1 Bladdernose Seal 48.2 Bladdernose Seal 33.2 Bladdernose Seal 38.4 Bladdernose Seal 26.0 Bladdernose Seal 50.0 Bladdernose Seal 42.6 Bladdernose Seal 47.0 Bladdernose Seal 41.6 The data were collected to determine whether muscle myoglobin differed between species. There are 2 variables. seal is the explanatory variable; it is categorical with 3 levels, Bladdernose Seal, Harbour Seal and Weddell Seal. myoglobin, a continuous variable, is the response. We can use the read_delim() function to import the data and visualise it with ggplot(). seal &lt;- read_delim(&quot;data-raw/seal.txt&quot;, delim = &quot; &quot;) # create a rough plot of the data ggplot(data = seal, aes(x = species, y = myoglobin)) + geom_violin() Harbour Seals seem to have higher myoglobin than the other two species and the variance in myoglobin for the three species looks about the same. Let’s create a summary of the data that will be useful for plotting later: seal_summary &lt;- seal %&gt;% group_by(species) %&gt;% summarise(mean = mean(myoglobin), std = sd(myoglobin), n = length(myoglobin), se = std/sqrt(n)) species mean std n se Bladdernose Seal 42.3 8.02 30 1.46 Harbour Seal 49.0 8.25 30 1.51 Weddell Seal 44.7 7.85 30 1.43 Our summary confirms that there are thirty individuals of each species and that highest mean is for Harbour Seals and the lowest is for Bladdernose Seals. The variance within each species is similar. 6.2 aov() output reminder The aov() function requires a model formula, myoglobin ~ species, in the familiar format. We also specify the data argument to indicate where the species and myoglobin variables can be found: mod &lt;- aov(data = seal, myoglobin ~ species) The output of the summary() function gives us an ANOVA test: summary(mod) # Df Sum Sq Mean Sq F value Pr(&gt;F) # species 2 692 346 5.35 0.0064 ** # Residuals 87 5627 65 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There was a significant difference in myoglobin concentration between seal species (ANOVA: \\(F\\) = 5.352; \\(d.f.\\) = 2, 87; \\(p\\) = 0.006). We need a post-hoc multiple comparison test to discover which pairs of means differ significantly. 6.3 Post-hoc testing for aov() A commonly applied multiple comparison test applied after an significant ANOVA result is the Tukey Honest Significant Difference test: TukeyHSD(mod) # Tukey multiple comparisons of means # 95% family-wise confidence level # # Fit: aov(formula = myoglobin ~ species, data = seal) # # $species # diff lwr upr p adj # Harbour Seal-Bladdernose Seal 6.69 1.74 11.646 0.005 # Weddell Seal-Bladdernose Seal 2.34 -2.61 7.296 0.499 # Weddell Seal-Harbour Seal -4.35 -9.30 0.602 0.097 The p-value, adjusted for multiple comparisons is given in the p adj column. In this case, only one of the three pairwise comparisons is significant. Harbour Seals, with the highest myoglobin concentrations (\\(\\bar{x} \\pm s.e.\\): 49.01 \\(\\pm\\) 1.507) ) were significantly higher than Bladdernose Seals with the lowest (\\(\\bar{x} \\pm s.e.\\): 42.316 \\(\\pm\\) 1.464). The comparisons being made are known as contrasts and this terminology will appear later. 6.4 One-way ANOVA as linear models The equation for a one-way ANOVA test is an extension of equation (5.1) for a t-test. It has the same form but additional parameters. If there are three groups, the model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i} \\tag{6.1} \\end{equation}\\] The parameter \\(\\beta_{0}\\), the intercept, is the value of the response when the categorical explanatory is at its “lowest” level. \\(X1_{i}\\) and \\(X2_{i}\\) are the dummy explanatory variables which take a value of 0 or 1 to toggle on and off the effects of \\(\\beta_{1}\\) and \\(\\beta_{2}\\) respectively. \\(\\beta_{1}\\) is the difference between the mean of the group represented by the intercept and the next group and \\(\\beta_{2}\\) is the difference between the mean of the group represented by the intercept and the group after that. An additional parameter and dummy variable are added for each additional group so for four groups the equation is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+\\beta_{3}X3_{i} \\tag{6.2} \\end{equation}\\] A graphical representation of the terms in a linear model when the explanatory variable is categorical with two groups is given in Figure 6.2. Figure 6.2: A linear model when the explanatory variable is categorical with four groups annotated with the terms used in linear modelling. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters are indicated: \\(\\beta_{0}\\) is the mean of group A; \\(\\beta_{1}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group B; \\(\\beta_{2}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group C; and \\(\\beta_{3}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group D. In this figure, \\(\\beta_{1}\\) and \\(\\beta_{2}\\) are positive and \\(\\beta_{3}\\) is negative. Compare to Figure 2.2. All the \\(\\beta\\) values are given relative to \\(\\beta_{0}\\). Their sign indicates whether a group mean is bigger (positive) or smaller (negative) than the intercept. 6.5 Applying and interpreting lm() The lm() function is applied to the seal example as follows: mod &lt;- lm(data = seal, myoglobin ~ species) Printing mod to the console gives us the estimated model parameters (coefficients): mod # # Call: # lm(formula = myoglobin ~ species, data = seal) # # Coefficients: # (Intercept) speciesHarbour Seal speciesWeddell Seal # 42.32 6.69 2.34 The first group of seal is Bladdernose Seal so \\(\\beta_{0}\\) is the mean of the Bladdernose seals. \\(\\beta_{1}\\) is the coefficient labelled speciesHarbour Seal and means when the variable species takes the value Harbour Seal, \\(\\beta_{1}\\) must be added to \\(\\beta_{0}\\). The last parameter, \\(\\beta_{2}\\), is the coefficient labelled speciesWeddell Seal and means when the variable species takes the value Weddell Seal, \\(\\beta_{2}\\) must be added to \\(\\beta_{0}\\). Thus, the mean myoglobin in Bladdernose seals is 42.316 kg g1, that in Harbour Seals is 42.316 + 6.694 = 49.01 kg g1 and in Weddell Seals is 42.316 + 2.344 = 44.66kg g1. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # lm(formula = myoglobin ~ species, data = seal) # # Residuals: # Min 1Q Median 3Q Max # -16.306 -5.578 -0.036 5.240 18.250 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 42.32 1.47 28.82 &lt;2e-16 *** # speciesHarbour Seal 6.69 2.08 3.22 0.0018 ** # speciesWeddell Seal 2.34 2.08 1.13 0.2620 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 8.04 on 87 degrees of freedom # Multiple R-squared: 0.11, Adjusted R-squared: 0.0891 # F-statistic: 5.35 on 2 and 87 DF, p-value: 0.00643 The Coefficients table gives the estimated \\(\\beta_{0}\\), \\(\\beta_{1}\\) and \\(\\beta_{2}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated mean of the Bladdernose seals is 42.316 \\(\\pm\\) 1.468 kg g1 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated difference between the Bladdernose and Harbour seals is 6.694 \\(\\pm\\) 2.077 and also differs significantly from zero (\\(p\\) = 0.002). The estimated difference between the Bladdernose and Weddell seals, 2.344 \\(\\pm\\) 2.077 kg g1, does not differ significantly from zero (\\(p\\) = 0.262). The fact that both parameters are positive tells us both have higher means than Bladdernose. The proportion of the variance in the omega which is explained by the model is 0.11 and this is a significant proportion of that variance (\\(p\\) = 0.006). This is the first time we have a model where the p-value for the model and the p-values for the \\(\\beta\\) parameters differ. This is because we are fitting two parameters after the intercept. Replacing the terms shown in Figure 5.1 with the values in this example gives us 5.2. Figure 6.3: The annotated model with the values from the Seal species example. The measured response values are in pink, the predictions are in green, and the residuals, are in blue. One example of a measured value, a predicted value and the residual is shown for an individual harbour seal. The estimated model parameters are indicated: \\(\\beta_{0}\\), the mean of the Bladdernose Seals, is 42.316 kg g1; \\(\\beta_{1}\\) is 6.694 thus the mean of Harbour Seals 42.316 + 6.694 = 49.01 kg g^-1; and \\(\\beta_{2}\\) is 2.344 thus the mean of Weddell Seals 42.316 + 2.344 = 49.01 kg g-1. Compare to Figure 6.2. 6.6 Getting predictions from the model We already have the predictions for all possible values of the explanatory variable because it is categorical. However, the code for using predict is included here, as it was in the last chapter, because it will make it easier to understand more complex examples later. We need to create a dataframe of values for which we want predictions and pass it as an argument to the predict() function. To create a dataframe with one column of Species values: predict_for &lt;- data.frame(species = c(&quot;Bladdernose Seal&quot;, &quot;Harbour Seal&quot;, &quot;Weddell Seal&quot;)) Remember! The variable and its values have to exactly match those in the model. The to get the predicted myoglobin content for the three species: predict_for$pred &lt;- predict(mod, newdata = predict_for) 6.7 Checking assumptions The two assumptions of the model can be checked using diagnostic plots. The Q-Q plot is obtained with: plot(mod, which = 2) The residual seem to be normally distributed. Let’s look at the Residuals vs Fitted plot: plot(mod, which = 1) The residuals are equally spread around a horizontal line; the assumptions seem to be met. 6.8 Post-hoc testing for lm() Instead of using the TukeyHSD() we will use the glht() (generalized linear hypothesis test) function from the multcomp package (Hothorn, Bretz, and Westfall 2008). This is function that can be applied more widely than TukeyHSD(). It provides multiple comparisons for linear models, generalised linear models and linear mixed effects models. This tremendous flexibility comes at some cost and the arguments for the glht() function are relatively complex. However, you don’t need a full understanding to be able to use it. glht() requires our species variable to be a factor so our first task is to transform that variable and rebuild our model: seal$species &lt;- factor(seal$species) mod &lt;- lm(data = seal, myoglobin ~ species) Then load the package: library(multcomp) We have to specify our contrasts as a matrix with the linfct (linear functions) argument and there is a multiple comparisons function, mcp(), to help. This is the whole command: mod_mc &lt;- glht(mod, linfct = mcp(species = &quot;Tukey&quot;)) You can read this as “do all of the pairwise comparisons between each species in the model mod using the Tukey test”. We view the results with summary(): summary(mod_mc) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = myoglobin ~ species, data = seal) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # Harbour Seal - Bladdernose Seal == 0 6.69 2.08 3.22 0.005 ** # Weddell Seal - Bladdernose Seal == 0 2.34 2.08 1.13 0.499 # Weddell Seal - Harbour Seal == 0 -4.35 2.08 -2.09 0.097 . # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) The results are the same as for using TukeyHSD() as we have done the same tests using a different function. You can see what a contrasts matrix looks like by looking at the linfct variable of the glht object. You don’t need it now but in the future you may need to specify your own constrasts matrices so let’s have a look to make a step towards understanding: mod_mc$linfct # (Intercept) speciesHarbour Seal # Harbour Seal - Bladdernose Seal 0 1 # Weddell Seal - Bladdernose Seal 0 0 # Weddell Seal - Harbour Seal 0 -1 # speciesWeddell Seal # Harbour Seal - Bladdernose Seal 0 # Weddell Seal - Bladdernose Seal 1 # Weddell Seal - Harbour Seal 1 # attr(,&quot;type&quot;) # [1] &quot;Tukey&quot; It is matrix with a column for each parameter, in order) and a row for each contrast containing only 0s, 1s and -1s. The rows are named. The numbers are how the model parameters are needed to make the contrast and these can be understood by considering how the group means relate to the parameters. Bladdernose mean is \\(\\beta_{0}\\) Harbour mean is \\(\\beta_{0} + \\beta_{1}\\) Weddell mean is \\(\\beta_{0} + \\beta_{2}\\) Therefore: * Harbour Seal - Bladdernose Seal is: \\(\\beta_{0} + \\beta_{1} - \\beta_{0} = \\beta_{1}\\) and there is a one in the speciesHarbour Seal column and zeros else where * Weddell Seal - Bladdernose Seal is: \\(\\beta_{0} + \\beta_{2} - \\beta_{0} = \\beta_{2}\\) and there is a one in the speciesWeddell Seal column and zeros else where * Weddell Seal - Harbour Seal is: \\(\\beta_{0} + \\beta_{2} - (\\beta_{0} + \\beta_{1}) = \\beta_{2} + \\beta_{1}\\) and there is a 1 in the the speciesWeddell Seal column and a -1 in the speciesHarbour Seal column. 6.9 Creating a figure #summarise the data ggplot() + geom_jitter(data = seal, aes(x = species, y = myoglobin), width = 0.25, colour = &quot;grey&quot;) + geom_errorbar(data = seal_summary, aes(x = species, ymin = mean, ymax = mean), width = .3) + geom_errorbar(data = seal_summary, aes(x = species, ymin = mean - se, ymax = mean + se), width = .5) + geom_segment(aes(x = 1, y = 71, xend = 3, yend = 71), size = 1) + geom_segment(aes(x = 1, y = 71, xend = 1, yend = 69), size = 1) + geom_segment(aes(x = 3, y = 71, xend = 3, yend = 69), size = 1) + annotate(&quot;text&quot;, x = 2, y = 73, label = &quot;**&quot;, size = 6) + scale_x_discrete(name = &quot;Species&quot;) + scale_y_continuous(name = expression(&quot;Myoglobin concentration g &quot;*Kg^{-1}), expand = c(0, 0), limits = c(0, 75)) + theme_classic() 6.10 Reporting the results to add: principle, sig, magnitude and diection of effects, test result, figure # res &lt;- summary(mod) # tval &lt;- res$coefficients[&quot;specieswild&quot;, &quot;t value&quot;] # df &lt;- res$df[2] There is a significant difference in myoglobin concentration between Seal species (ANOVA: \\(F\\) = 5.352; \\(d.f.\\) = 2, 87; \\(p\\) = 0.006). Post-hoc testing revealed that difference to be between the Harbour Seal with the highest myoglobin concentrations (\\(\\bar{x} \\pm s.e.\\): 49.01 \\(\\pm\\) 1.507) ) and the Bladdernose Seal with the lowest (\\(\\bar{x} \\pm s.e.\\): 42.316 \\(\\pm\\) 1.464). See figure 6.4. Figure 6.4: Muscle myoglobin content of three seal species. References "],
["two-way-anova-revisit.html", "Chapter 7 Two-way ANOVA revisited 7.1 Introduction to the example 7.2 aov() output reminder 7.3 Post-hoc testing for aov 7.4 Applying and interpreting lm() 7.5 Getting predictions from the model 7.6 Link to Chapter 2.1 7.7 Checking assumptions 7.8 Post-hoc testing for lm() 7.9 Creating a figure 7.10 Reporting the results", " Chapter 7 Two-way ANOVA revisited In this chapter we turn our attention to designs with two categorical explanatory variables. We first use the familiar aov() function to carry out a two-way ANOVA and then use our understanding to help us interpret the output of lm(). We will also make predictions from the model and report on our results. 7.1 Introduction to the example A group of amateur conchologists have collected live specimens of two species of rough periwinkle (intertidal, gastropod molluscs) from sites in northern England in the Spring (1) and Summer (2). Among other variables, they take a measure of the gut parasite load. Number of parasites is related to the number of parasites seen on a slide of gut contents and larger numbers indicate a higher parasite load. The data are in S periwinkle.txt. periwinkle &lt;- read_delim(&quot;data-raw/periwinkle.txt&quot;, delim = &quot;\\t &quot;) periwinkle$species &lt;- factor(periwinkle$species) periwinkle$season &lt;- factor(periwinkle$season) Do a quick plot of the data. We have two explanatory variables: one can be mapped to the x-axis and the is mapped to a different aesthetic. We have used fill. ggplot(data = periwinkle, aes(x = season, y = para, fill = species)) + geom_violin() peri_summary &lt;- periwinkle %&gt;% group_by(season, species) %&gt;% summarise(mean = mean(para), sd = sd(para), n = length(para), se = sd / sqrt(n)) season species mean sd n se Spring Littorina nigrolineata 63.8 11.92 25 2.38 Spring Littorina saxatilis 56.5 8.88 25 1.78 Summer Littorina nigrolineata 69.4 11.44 25 2.29 Summer Littorina saxatilis 72.9 11.24 25 2.25 mainly incomplete from here: code and output, little commentary 7.2 aov() output reminder mod &lt;- aov(data = periwinkle, para ~ season * species) summary(mod) # Df Sum Sq Mean Sq F value Pr(&gt;F) # season 1 3058 3058 25.58 2e-06 *** # species 1 90 90 0.75 0.387 # season:species 1 724 724 6.05 0.016 * # Residuals 96 11477 120 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 res &lt;- summary(mod)[[1]] df_seas &lt;- res$Df[1] df_sp &lt;- res$Df[2] df_seasxsp &lt;- res$Df[3] df_err &lt;- res$Df[4] fval_seas &lt;- res$`F value`[1] fval_sp &lt;- res$`F value`[2] fval_seasxsp &lt;- res$`F value`[3] if (res$`Pr(&gt;F)`[1] &lt; 0.001) { pval_seas = &quot;&lt; 0.001&quot; } if (res$`Pr(&gt;F)`[1] &gt; 0.001) { pval_seas = paste(&quot;=&quot;, round(res$`Pr(&gt;F)`[1], 3)) } if (res$`Pr(&gt;F)`[2] &lt; 0.001) { pval_sp = &quot;&lt; 0.001&quot; } if (res$`Pr(&gt;F)`[2] &gt; 0.001) { pval_sp = paste(&quot;=&quot;, round(res$`Pr(&gt;F)`[2], 3)) } if (res$`Pr(&gt;F)`[3] &lt; 0.001) { pval_seasxsp = &quot;&lt; 0.001&quot; } if (res$`Pr(&gt;F)`[3] &gt; 0.001) { pval_seasxsp = paste(&quot;=&quot;, round(res$`Pr(&gt;F)`[3], 3)) } There was a significantly greater number of parasites in the Summer than the Spring (ANOVA: \\(F\\) = 25.58; \\(d.f.\\) = 1, 96; \\(p\\) &lt; 0.001). There was no difference between the species when averaged across the season but there was significant interaction (ANOVA: \\(F\\) = 6.053; \\(d.f.\\) = 1, 96; \\(p\\) = 0.016) between season and species with higher numbers infecting L.nigrolineata in the Spring whilst L.saxatilis was more heavily parasitized in the Summer. We need a post-hoc test to discover which comparisons are significant. 7.3 Post-hoc testing for aov TukeyHSD(mod) # Tukey multiple comparisons of means # 95% family-wise confidence level # # Fit: aov(formula = para ~ season * species, data = periwinkle) # # $season # diff lwr upr p adj # Summer-Spring 11.1 6.72 15.4 0 # # $species # diff lwr upr p adj # Littorina saxatilis-Littorina nigrolineata -1.9 -6.24 2.44 0.387 # # $`season:species` # diff lwr # Summer:Littorina nigrolineata-Spring:Littorina nigrolineata 5.68 -2.41 # Spring:Littorina saxatilis-Spring:Littorina nigrolineata -7.28 -15.37 # Summer:Littorina saxatilis-Spring:Littorina nigrolineata 9.16 1.07 # Spring:Littorina saxatilis-Summer:Littorina nigrolineata -12.96 -21.05 # Summer:Littorina saxatilis-Summer:Littorina nigrolineata 3.48 -4.61 # Summer:Littorina saxatilis-Spring:Littorina saxatilis 16.44 8.35 # upr p adj # Summer:Littorina nigrolineata-Spring:Littorina nigrolineata 13.766 0.263 # Spring:Littorina saxatilis-Spring:Littorina nigrolineata 0.806 0.093 # Summer:Littorina saxatilis-Spring:Littorina nigrolineata 17.246 0.020 # Spring:Littorina saxatilis-Summer:Littorina nigrolineata -4.874 0.000 # Summer:Littorina saxatilis-Summer:Littorina nigrolineata 11.566 0.675 # Summer:Littorina saxatilis-Spring:Littorina saxatilis 24.526 0.000 Note that the output is wrapped because the names associated with each comparison, for example “Summer:Littorina nigrolineata-Spring:Littorina nigrolineata”, are quite long. L.saxatilis has fewer parasites in the spring than L.nigrolineata (\\(p\\) = 0.02) but this rises significantly in Summer (\\(p\\) &lt; 0.001) while that of L.nigrolineata does not. 7.4 Applying and interpreting lm() mod &lt;- lm(data = periwinkle, para ~ season * species) mod # # Call: # lm(formula = para ~ season * species, data = periwinkle) # # Coefficients: # (Intercept) # 63.76 # seasonSummer # 5.68 # speciesLittorina saxatilis # -7.28 # seasonSummer:speciesLittorina saxatilis # 10.76 The mean of L.nigrolineata in the Spring is (63.76). It is the intercept (\\(\\beta_{0}\\)) because “Littorina nigrolineata” comes before “Littorina saxatilis” and “Spring” comes before “Summer” in the alphabet. The value labelled “seasonSummer” is the difference between the intercept and the L.nigrolineata in the Summer. It indicates that, holding all other variables constant (in this case, species), if you change the season variable to Summer you have to add 5.68 to 63.76 to get the mean of L.nigrolineata in the Summer. The value labelled “speciesLittorina saxatilis” is also relative to the intercept. Holding all other variables constant (in this case, season), if you change the species variable to Littorina saxatilis you have to add -7.28 to 63.76 to get the mean of L.saxatilis in the Spring. Similarly, the value labelled “seasonSummer:speciesLittorina saxatilis” is relative to the intercept but the estimate, 10.76 is what you must add additionally. If you change the season variable to Summer and the species variable to Littorina saxatilis you have to add 5.68 (the effect of season), -7.28 (the effect of species) and 10.76 (the additional effect) to to 63.76 to get the mean of L.saxatilis in the Summer. 7.5 Getting predictions from the model You must have all the combination. these are the means for two categorical variables predictions &lt;- data.frame(species = peri_summary$species, season = peri_summary$season) predictions$pred &lt;- predict(mod, newdata = predictions) 7.6 Link to Chapter 2.1 Replacing the terms shown in Figure 2.2 with the values in this example gives us ??. 7.7 Checking assumptions plot(mod, which = 2) plot(mod, which = 1) shapiro.test(mod$res) # # Shapiro-Wilk normality test # # data: mod$res # W = 1, p-value = 0.3 7.8 Post-hoc testing for lm() library(multcomp) generic example. define linfct, mcp multiple comparison procedures can be obtained by fitting the so-called cell-means model based on a new factor derived as the interaction of species and season: periwinkle$seasxspp &lt;- interaction(periwinkle$season, periwinkle$species) mod2 &lt;- lm(data = periwinkle, para ~ seasxspp) mod2_mc &lt;- glht(mod2, linfct = mcp(seasxspp = &quot;Tukey&quot;)) summary(mod2_mc) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = para ~ seasxspp, data = periwinkle) # # Linear Hypotheses: # Estimate # Summer.Littorina nigrolineata - Spring.Littorina nigrolineata == 0 5.68 # Spring.Littorina saxatilis - Spring.Littorina nigrolineata == 0 -7.28 # Summer.Littorina saxatilis - Spring.Littorina nigrolineata == 0 9.16 # Spring.Littorina saxatilis - Summer.Littorina nigrolineata == 0 -12.96 # Summer.Littorina saxatilis - Summer.Littorina nigrolineata == 0 3.48 # Summer.Littorina saxatilis - Spring.Littorina saxatilis == 0 16.44 # Std. Error # Summer.Littorina nigrolineata - Spring.Littorina nigrolineata == 0 3.09 # Spring.Littorina saxatilis - Spring.Littorina nigrolineata == 0 3.09 # Summer.Littorina saxatilis - Spring.Littorina nigrolineata == 0 3.09 # Spring.Littorina saxatilis - Summer.Littorina nigrolineata == 0 3.09 # Summer.Littorina saxatilis - Summer.Littorina nigrolineata == 0 3.09 # Summer.Littorina saxatilis - Spring.Littorina saxatilis == 0 3.09 # t value # Summer.Littorina nigrolineata - Spring.Littorina nigrolineata == 0 1.84 # Spring.Littorina saxatilis - Spring.Littorina nigrolineata == 0 -2.35 # Summer.Littorina saxatilis - Spring.Littorina nigrolineata == 0 2.96 # Spring.Littorina saxatilis - Summer.Littorina nigrolineata == 0 -4.19 # Summer.Littorina saxatilis - Summer.Littorina nigrolineata == 0 1.13 # Summer.Littorina saxatilis - Spring.Littorina saxatilis == 0 5.32 # Pr(&gt;|t|) # Summer.Littorina nigrolineata - Spring.Littorina nigrolineata == 0 0.263 # Spring.Littorina saxatilis - Spring.Littorina nigrolineata == 0 0.093 . # Summer.Littorina saxatilis - Spring.Littorina nigrolineata == 0 0.020 * # Spring.Littorina saxatilis - Summer.Littorina nigrolineata == 0 &lt;0.001 *** # Summer.Littorina saxatilis - Summer.Littorina nigrolineata == 0 0.675 # Summer.Littorina saxatilis - Spring.Littorina saxatilis == 0 &lt;0.001 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) 7.9 Creating a figure #summarise the data ggplot() + geom_point(data = periwinkle, aes(x = season, y = para, colour = species), position = position_jitterdodge(dodge.width = 1, jitter.width = 0.4, jitter.height = 0), size = 2) + geom_errorbar(data = peri_summary, aes(x = season, ymin = mean - se, ymax = mean + se, group = species), width = 0.4, size = 1, position = position_dodge(width = 1)) + geom_errorbar(data = peri_summary, aes(x = season, ymin = mean, ymax = mean, group = species), width = 0.3, size = 1, position = position_dodge(width = 1) ) + scale_x_discrete(name = &quot;Season&quot;) + scale_y_continuous(name = &quot;Number of parasites&quot;, expand = c(0, 0), limits = c(0, 128)) + scale_colour_manual(values = pal4[1:2]) + # Spring:Littorina nigrolineata-Summer:Littorina saxatilis * annotate(&quot;segment&quot;, x = 1.25, xend = 1.75, y = 110, yend = 110, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.25, xend = 1.25, y = 110, yend = 105, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.75, xend = 1.75, y = 110, yend = 105, colour = &quot;black&quot;) + annotate(&quot;text&quot;, x = 1.5, y = 112, label = &quot;***&quot;, size = 6) + # Summer:Littorina nigrolineata-Spring:Littorina saxatilis: *** annotate(&quot;segment&quot;, x = 1.25, xend = 0.75, y = 90, yend = 90, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.25, xend = 1.25, y = 90, yend = 85, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 0.75, xend = 0.75, y = 90, yend = 85, colour = &quot;black&quot;) + annotate(&quot;text&quot;, x = 1, y = 92, label = &quot;**&quot;, size = 6) + # Summer:Littorina saxatilis-Spring:Littorina saxatilis: *** annotate(&quot;segment&quot;, x = 0.75, xend = 1.75, y = 120, yend = 120, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 0.75, xend = 0.75, y = 120, yend = 115, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.75, xend = 1.75, y = 120, yend = 115, colour = &quot;black&quot;) + annotate(&quot;text&quot;, x = 1.25, y = 123, label = &quot;***&quot;, size = 6) + theme_classic() + theme(legend.title = element_blank(), legend.position = c(0.85, 0.98)) 7.10 Reporting the results to add: principle, sig, magnitude and diection of effects, test result, figure See figure ??. "],
["glm-overview.html", "Chapter 8 Overview 8.1 Model fitting 8.2 More than one explanatory variable 8.3 Generalised linear models in R 8.4 Reporting", " Chapter 8 Overview General Linear models predict the value of a normally distributed response from a linear combination of predictor variables. The Generalised Linear Model is an extension of the General Linear model for situations where the response variable does not follow the normal distribution. It can, instead, come from a large family of distributions known as the Exponential family which includes the normal, Poisson and binomial distributions. It generalises by allowing the linear model to be related to the response variable by a something we call a link function. When you have a single explanatory variable, that model is: \\[\\begin{equation} function(E(y_{i}))=\\beta_{0}+\\beta_{1}X1_{i} \\tag{8.1} \\end{equation}\\] Where: \\(y\\) is the response variable and \\(X1\\) is the explanatory variable. \\(i\\) is the index so \\(X1_{i}\\) is the \\(i\\)th value of \\(X1\\) \\(E(y_{i})\\) is the expected value of \\(y\\) for the \\(i\\)th value of \\(X1\\). \\(function()\\) is the link function \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the coefficients in the model. The link function is a transformation applied to the expected value of the response to link it to the explanatory variables. If you measured the response for a particular value of \\(x\\) very many times there would be some distribution of those responses. Instead of that distribution having to be normal, it can come from a different distribution such has the Poisson or Binomial distributions. As a consequence, the residuals also come from that distribution. The fact that the residuals can follow a distribution other than normal also means the the variance no longer has to be homogeneous but can change for the values of x. 8.1 Model fitting A very important difference between the general linear model (applied with lm()) and the generalised linear model (applied with glm()) is that the estimates of the coefficients are given on the scale of the link function. You cannot just read the predicted response at \\(x=0\\) from \\(\\beta_{0}\\). You have to invert the link function to express the coefficients in terms of the response. Another important difference between these models is in the measure of fit and the test on that measure of fit. The parameters of General linear models are chosen to minimise the sum of the squared residuals and use \\(R^2\\), the proportion of variance explained, and a variance ratio test, the F-test. In GLMS we maximise the log-likelihood, \\(l\\), of our model to choose our parameter values. This is known as maximum likelihood estimation. The measure of fit used in GLMs is deviance where deviance is \\(-2l\\). Low deviance means a good fit and high deviance a worse fit. Thus maximum likelihood estimation can be thought of as minimising deviance. The test again compares deviance predictions by the model and predictions from the intercept alone (the overall mean). This deviance in predictions from the intercept alone is called the Null deviance. Instead of considering the size of the \\(R^2\\) we consider the reduction in deviance between the null model and the residual deviance (the deviance left over after the model fit). You can consider the null deviance to play the same role as \\(SST\\). The difference between the null model and the full model has a chi-squared distribution and the test of whether the model is good overall is a chi-squared test of deviance in the simailar way that general linear model use the \\(F\\) variance ration test. The deviance plays the same role in GLMs that SSE plays in the general linear model. It is how much variation is unexplained by the model AIC is the equivalent of adjusted \\(R^2\\), Poisson GLMs are used when our response is a count. They are also known as GLMs with Poisson errors or Poisson regression. The link function is \\(ln\\), a function you probably know. This means the coefficients have to be exponentiated using exp() to get predicted counts because exp() is the inverse of log() Binomial GLMs are used when our response is binary, a zero or one, or a proportion. They are also known as GLMs with binomial errors, binomial regression or logistic regression. The link function is \\(logit\\), a function you may not have heard of before. We will discuss what that function looks like in …… to add: illustrations of good fit and bad fit 8.2 More than one explanatory variable 8.3 Generalised linear models in R 8.3.1 Building and viewing Poisson and binomial generalised linear models (and others) can be carried out with the glm() function in R. It uses the same method for specifying the model. When you have one explanatory variable the command is: glm(data = dataframe, response ~ explanatory, family = distribution(link = linkfunction)) For a Poisson GLM this is: glm(data = dataframe, response ~ explanatory, family = poisson(link = log)) In R, the log() function compute natural logarithms (i.e., logs to the base \\(e\\) by default For a binomial distribution this is: glm(data = dataframe, response ~ explanatory, family = binomial(link = “logit”)) The model formula can be developed in the same way we’ve seen previously. When you have two explanatory variable we add the second explanatory variable to the formula using a + or a *. The command is: lm(data = dataframe, response ~ explanatory1 + explanatory2, family = distribution(link = linkfunction)) or lm(data = dataframe, response ~ explanatory1 * explanatory2, family = distribution(link = linkfunction)) A model with explanatory1 + explanatory2 considers the effects of the two variables independently. A model with explanatory1 + explanatory2 considers the effects of the two variables and any interaction between them. We usually assign the output of glm() commands to an object and view it with summary(). The typical workflow would be: mod &lt;- glm(data = dataframe, response ~ explanatory, family = distribution(link = linkfunction)) summary(mod) glm() can be used to perform tests using the Generalised Linear Model including Poisson and Binomial regression. Elements of the glm() object include the estimated coefficients, the predicted values and the residuals can be accessed with mod$coeffients, mod$fitted.values and mod$residuals respectively. 8.3.2 Getting predictions mod$fitted.values gives the predicted values for the explanatory variable values actually used in the experiment, i.e., there is a prediction for each row of data. These are given on the scale of the response. This means they will be predicted counts for Poisson GLMs and predicted probabilities for Binomial GLMs. To get predictions for a different set of values make a dataframe of the different set of values and use the predict() function. When using the predict() function we have to specify that we want our predictions on the scale of the response rather than the scale of the link function using the type argument. The typical workflow would be: predict_for &lt;- data.frame(explanatory = values) predict_for$pred &lt;- predict(mod, newdata = predict_for, type = “response”) 8.3.3 Checking assumptions # plot(mod, which = 2) # plot(mod, which = 1) 8.4 Reporting to add, same principles apply "],
["pois-glm-overview.html", "Chapter 9 Poisson GLM overview 9.1 When explanatory is categorical 9.2 More than one explanatory", " Chapter 9 Poisson GLM overview When a response variable is the count of things it can often not be approximated by a normal distribution. Instead it follows a Poisson distribution. Such variables are always positive - they range from 0 to \\(\\infty\\). A Poisson GLM is also known as Poisson regression. The link function used in a Poisson GLM is the natural logarithm, \\(ln\\). When you have a single explanatory variable, that model is: \\[\\begin{equation} ln(E(y_{i}))=\\beta_{0}+\\beta_{1}X1_{i} \\tag{9.1} \\end{equation}\\] This means that the model estimates are logged to the base \\(e\\) and and the inverse function, exp() must be applied to them to interpret them in terms of the response. In other words, to make predictions about the expected value of the response we need to exponentiate the coefficients. \\[\\begin{equation} E(y_{i})=exp(\\beta_{0}+\\beta_{1}X1_{i}) \\tag{9.2} \\end{equation}\\] or \\[\\begin{equation} E(y_{i})=exp(\\beta_{0}) \\times exp(\\beta_{1})^{X1_{i}} \\tag{9.3} \\end{equation}\\] Just like examples of general linear models with a single explanatory variable, there are two parameters in this model, \\(\\beta_{0}\\) and \\(\\beta_{0}\\) and their meaning is similar. \\(\\beta_{0}\\) is the log of the expected \\(y\\) when \\(x\\) is zero - the intercept. The log of \\(\\beta_{1}\\) is not the amount you add to \\(y\\) for each unit change in \\(x\\) but the amount by which multiply. This means the model is a curve. If \\(\\beta_{1}\\) is positive, \\(exp(\\beta_{1})\\) is greater than one and \\(y\\) increases as \\(x\\) increases; If \\(\\beta_{1}\\) is negative, \\(exp(\\beta_{1})\\) is less than one and \\(y\\) decreases as \\(x\\) increases. See Figure 9.1 for an illustration of the curve for positive and negative \\(\\beta_{1}\\). Figure 9.1: Data fitted with a Poisson GLM. blah blah blah See Figure 9.2 for a graphical representation of generalised linear model terms. Figure 9.2: A Generalised linear model with Poisson distributed errors. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) must be exponentiated to be interpreted on the scale of the response. When \\(x=0\\) we predict the number of \\(y\\) to be \\(exp(\\beta_{0})\\). For each unit of \\(x\\), the number of y changes by a factor of \\(exp(\\beta_{1})\\) 9.1 When explanatory is categorical one explanatory use chi-squared 9.2 More than one explanatory "],
["pois-glm-single-cont.html", "Chapter 10 Single continuous explanatory 10.1 Introduction to the example 10.2 Applying and interpreting glm() 10.3 Getting predictions from the model 10.4 Checking assumptions 10.5 Creating a figure 10.6 Reporting the results.", " Chapter 10 Single continuous explanatory 10.1 Introduction to the example The number of cases of cancer reported by a clinic and its distance, in kilometres, from a nuclear plant were recorded and the data are in cases.txt. Researchers wanted to know if proximity to the nuclear power plant influenced the incidence of cancer. Bear in mind this is not great epidemiology - there would be very many other factors influencing the occurrence and reporting of cancer cases at a clinic. cancers distance 0 154.37 0 93.14 4 3.83 0 60.83 0 142.61 0 164.72 0 135.92 1 79.92 0 112.71 0 101.76 2 59.62 0 128.07 2 17.17 1 24.81 1 103.42 0 112.70 0 143.96 1 48.77 1 82.20 1 57.53 1 12.75 0 64.47 1 68.78 1 133.40 0 98.94 0 40.87 0 151.82 4 35.15 0 97.10 0 131.44 0 102.02 0 116.77 1 28.79 0 52.63 2 23.15 1 68.13 0 146.93 1 87.98 0 147.30 1 132.67 0 164.21 1 72.67 0 22.81 There are 2 variables: the response, cancers, is the number of cancer cases reported at a clinic and distance, gives the clinic’s distance from the nuclear plant. We will import the data with the read_table2() function and plot it with ggplot(). cases &lt;- read_table2(&quot;data-raw/cases.txt&quot;) # a default scatter plot of the data ggplot(data = cases, aes(x = distance, y = cancers)) + geom_point() Most of the clinics reporting no cases seem to be more distance from the nuclear plant and those reporting the highest numbers are within 50km. 10.2 Applying and interpreting glm() We build a generalised linear model of the number of cases explained by the distance with the glm() function as follows: mod &lt;- glm(data = cases, cancers ~ distance, family = poisson) Printing mod to the console gives us the estimated model parameters: mod # # Call: glm(formula = cancers ~ distance, family = poisson, data = cases) # # Coefficients: # (Intercept) distance # 1.0192 -0.0215 # # Degrees of Freedom: 42 Total (i.e. Null); 41 Residual # Null Deviance: 54.5 # Residual Deviance: 31.8 AIC: 78.2 \\(\\beta_{0}\\) is labelled “(Intercept)” and \\(\\beta_{1}\\) is labelled “distance”. Thus the equation of the line is: \\(ln(cancers)\\) = 1.019 \\(\\times\\) -0.021\\(\\times distance\\) The fact that the estimate for distance (-0.021) is negative tells us that as distance increases, the number of cancers reported goes down. These estimates are on the scale of the link function, that is, they are logged (to the base e, natural logs) in this case. To understand the parameters the on the scale of the response we apply the inverse of the \\(ln\\) function, the exp() function exp(mod$coefficients) # (Intercept) distance # 2.771 0.979 So \\(cancers\\) = 2.771 \\(\\times\\) 0.979\\(^{distance}\\) The model predicts there will be 2.771 cancers at a clinic at no distance from the power plant. Recall that for a linear model with one predictor, the second estimate is the amount added to the intercept when the predictor changes by one value. Since this is glm with a log link, the value of 0.979 is amount the intercept is multiplied by for each unit increase of distance. Thus the model predicts there will be 2.771 \\(\\times\\) 0.979 = 2.712 cancers 1 km away and 2.771 \\(\\times\\) 0.979 \\(\\times\\) 0.979 = 2.654 cancers 2 km away. That is: \\(\\beta_{0}\\) \\(\\times\\) \\(\\beta_{0}^n\\) mm at \\(n\\) km away. You can work these out either by exponentiating the coefficients and then multiplying the results or by adding the coefficients and exponentiating Exponentiate coefficients then multiply: # 1km away exp(b0) * exp(b1) # [1] 2.71 # 2km away exp(b0) * exp(b1) * exp(b1) # [1] 2.65 # 10km away exp(b0) * exp(b1)^10 # [1] 2.23 Add the coefficients then exponentiate the sum: # 1km away exp(b0 + b1) # [1] 2.71 # 2km away exp(b0 + b1 + b1) # [1] 2.65 # 10km away exp(b0 + 10*b1) # [1] 2.23 Usually, we use the predict() function to make predictions for particular distances. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # glm(formula = cancers ~ distance, family = poisson, data = cases) # # Deviance Residuals: # Min 1Q Median 3Q Max # -1.842 -0.744 -0.483 0.421 1.893 # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) 1.01917 0.30871 3.30 0.00096 *** # distance -0.02150 0.00503 -4.27 2e-05 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # (Dispersion parameter for poisson family taken to be 1) # # Null deviance: 54.522 on 42 degrees of freedom # Residual deviance: 31.790 on 41 degrees of freedom # AIC: 78.16 # # Number of Fisher Scoring iterations: 5 The Coefficients table gives the estimated \\(\\beta_{0}\\) and \\(\\beta_{1}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated value for the intercept is 1.019 \\(\\pm\\) 0.309 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated value for the slope is -0.021 \\(\\pm\\) 0.005, also differs significantly from zero (\\(p\\) &lt; 0.001). The three lines at the bottom of the output give information about the fit of the model to the data. The Multiple R-squared gives the proportion of the variance in the response which is explained by the model. In our case, “r rsq” of the variance in mandible length is explained by the model and this is a significant proportion of that variance (\\(p\\) “r modelp”). To get a test of the model overall anova(mod, test = &quot;Chisq&quot;) # Analysis of Deviance Table # # Model: poisson, link: log # # Response: cancers # # Terms added sequentially (first to last) # # # Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) # NULL 42 54.5 # distance 1 22.7 41 31.8 1.9e-06 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 pchisq(22.732, 1, lower.tail = F) # [1] 1.86e-06 10.3 Getting predictions from the model The predict() function returns the predicted values of the response. To add a column of predicted values to the dataframe: we need to specify they should be on the scale of the responses, no the link function scale. cases$pred &lt;- predict(mod, type = &quot;response&quot;) mod$fitted.values # 1 2 3 4 5 6 7 8 9 10 11 # 0.1004 0.3742 2.5519 0.7495 0.1292 0.0803 0.1492 0.4972 0.2457 0.3109 0.7692 # 12 13 14 15 16 17 18 19 20 21 22 # 0.1766 1.9157 1.6256 0.3000 0.2458 0.1255 0.9713 0.4734 0.8046 2.1067 0.6931 # 23 24 25 26 27 28 29 30 31 32 33 # 0.6317 0.1575 0.3304 1.1510 0.1060 1.3016 0.3437 0.1643 0.3092 0.2252 1.4923 # 34 35 36 37 38 39 40 41 42 43 # 0.8939 1.6846 0.6406 0.1178 0.4181 0.1168 0.1600 0.0812 0.5811 1.6970 This gives predictions for the actual x values used. If you want predicts for other values of x you need to creating a data frame of the x values from which you want to predict predictions &lt;- data.frame(distance = seq(0, 180, 10)) predictions$pred &lt;- predict(mod, newdata = predictions, type = &quot;response&quot;) 10.4 Checking assumptions 10.5 Creating a figure ggplot(data = cases, aes(x = distance, y = cancers)) + geom_point() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;), se = FALSE, colour = &quot;black&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 200), name = &quot;Distance (km) of clinic from plant&quot;) + scale_y_continuous(expand = c(0, 0.03), limits = c(0, 5), name = &quot;Number of reported cancers&quot;) + theme_classic() 10.6 Reporting the results. The number of case reported by a clinic See figure ??. Figure 10.1: Incidence of cancer cases reported at clinic by it distance from the nuclear plant. "],
["pois-glm-two-cont.html", "Chapter 11 Two explanatory variables 11.1 Introduction to the example 11.2 Applying and interpreting glm()", " Chapter 11 Two explanatory variables 11.1 Introduction to the example The number of cases of cancer reported by a clinic and its distance, in kilometres, from a nuclear plant were recorded and the data are in cases.txt. Researchers wanted to know if proximity to the nuclear power plant influenced the incidence of cancer. Bear in mind this is not great epidemiology - there would be very many other factors influencing the occurrence and reporting of cancer cases at a clinic. cancers distance 0 154.37 0 93.14 4 3.83 0 60.83 0 142.61 0 164.72 0 135.92 1 79.92 0 112.71 0 101.76 2 59.62 0 128.07 2 17.17 1 24.81 1 103.42 0 112.70 0 143.96 1 48.77 1 82.20 1 57.53 1 12.75 0 64.47 1 68.78 1 133.40 0 98.94 0 40.87 0 151.82 4 35.15 0 97.10 0 131.44 0 102.02 0 116.77 1 28.79 0 52.63 2 23.15 1 68.13 0 146.93 1 87.98 0 147.30 1 132.67 0 164.21 1 72.67 0 22.81 There are 2 variables: the response, cancers, is the number of cancer cases reported at a clinic and distance, gives the clinic’s distance from the nuclear plant. We will import the data with the read_table2() function and plot it with ggplot(). cases &lt;- read_table2(&quot;data-raw/cases.txt&quot;) # a default scatter plot of the data ggplot(data = cases, aes(x = distance, y = cancers)) + geom_point() Most of the clinics reporting no cases seem to be more distance from the nuclear plant and those reporting the highest numbers are within 50km. 11.2 Applying and interpreting glm() We build a generalised linear model of the number of cases explained by the distance with the glm() function as follows: mod &lt;- glm(data = cases, cancers ~ distance, family = poisson) Printing mod to the console gives us the estimated model parameters: mod # # Call: glm(formula = cancers ~ distance, family = poisson, data = cases) # # Coefficients: # (Intercept) distance # 1.0192 -0.0215 # # Degrees of Freedom: 42 Total (i.e. Null); 41 Residual # Null Deviance: 54.5 # Residual Deviance: 31.8 AIC: 78.2 \\(\\beta_{0}\\) is labelled “(Intercept)” and \\(\\beta_{1}\\) is labelled “distance”. Thus the equation of the line is: \\(ln(cancers)\\) = 1.019 \\(\\times\\) -0.021\\(\\times distance\\) The fact that the estimate for distance (-0.021) is negative tells us that as distance increases, the number of cancers reported goes down. These estimates are on the scale of the link function, that is, they are logged (to the base e, natural logs) in this case. To understand the parameters the on the scale of the response we apply the inverse of the \\(ln\\) function, the exp() function exp(mod$coefficients) # (Intercept) distance # 2.771 0.979 So \\(cancers\\) = 2.771 \\(\\times\\) 0.979\\(^{distance}\\) The model predicts there will be 2.771 cancers at a clinic at no distance from the power plant. Recall that for a linear model with one predictor, the second estimate is the amount added to the intercept when the predictor changes by one value. Since this is glm with a log link, the value of 0.979 is amount the intercept is multiplied by for each unit increase of distance. Thus the model predicts there will be 2.771 \\(\\times\\) 0.979 = 2.712 cancers 1 km away and 2.771 \\(\\times\\) 0.979 \\(\\times\\) 0.979 = 2.654 cancers 2 km away. That is: \\(\\beta_{0}\\) \\(\\times\\) \\(\\beta_{0}^n\\) mm at \\(n\\) km away. You can work these out either by exponentiating the coefficients and then multiplying the results or by adding the coefficients and exponentiating Exponentiate coefficients then multiply: # 1km away exp(b0) * exp(b1) # [1] 2.71 # 2km away exp(b0) * exp(b1) * exp(b1) # [1] 2.65 # 10km away exp(b0) * exp(b1)^10 # [1] 2.23 Add the coefficients then exponentiate the sum: # 1km away exp(b0 + b1) # [1] 2.71 # 2km away exp(b0 + b1 + b1) # [1] 2.65 # 10km away exp(b0 + 10*b1) # [1] 2.23 Usually, we use the predict() function to make predictions for particular distances. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # glm(formula = cancers ~ distance, family = poisson, data = cases) # # Deviance Residuals: # Min 1Q Median 3Q Max # -1.842 -0.744 -0.483 0.421 1.893 # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) 1.01917 0.30871 3.30 0.00096 *** # distance -0.02150 0.00503 -4.27 2e-05 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # (Dispersion parameter for poisson family taken to be 1) # # Null deviance: 54.522 on 42 degrees of freedom # Residual deviance: 31.790 on 41 degrees of freedom # AIC: 78.16 # # Number of Fisher Scoring iterations: 5 "],
["bino-glm-overview.html", "Chapter 12 Binomial GLM overview", " Chapter 12 Binomial GLM overview For the binomial and quasibinomial families the response can be specified in one of three ways: As a factor: ‘success’ is interpreted as the factor not having the first level (and hence usually of having the second level). As a numerical vector with values between 0 and 1, interpreted as the proportion of successful cases (with the total number of cases given by the weights). As a two-column integer matrix: the first column gives the number of successes and the second the number of failures. The quasibinomial and quasipoisson families differ from the binomial and poisson families only in that the dispersion parameter is not fixed at one, so they can model over-dispersion. For the binomial case see McCullagh and Nelder (1989, pp. 124–8). Although they show that there is (under some restrictions) a model with variance proportional to mean as in the quasi-binomial model, note that glm does not compute maximum-likelihood estimates in that model. When a response variable is the count of things it can often not be approximated by a normal distribution. Instead it follows a Poisson distribution. Such variables are always positive - they range from 0 to \\(\\infty\\). A Poisson GLM is also known as Poisson regression. The link function used in a Poisson GLM is the natural logarithm, \\(ln\\). When you have a single explanatory variable, that model is: \\[\\begin{equation} ln(E(y_{i}))=\\beta_{0}+\\beta_{1}X1_{i} \\tag{9.1} \\end{equation}\\] This means that the model estimates are logged to the base \\(e\\) and and the inverse function, exp() must be applied to them to interpret them in terms of the response. In other words, to make predictions about the expected value of the response we need to exponentiate the coefficients. \\[\\begin{equation} E(y_{i})=exp(\\beta_{0}+\\beta_{1})X1_{i} \\tag{9.2} \\end{equation}\\] or \\[\\begin{equation} E(y_{i})=exp(\\beta_{0}) \\times exp(\\beta_{1})X1_{i} \\tag{9.3} \\end{equation}\\] Just like examples of general linear models with a single explanatory variable, there are two parameters in this model, \\(\\beta_{0}\\) and \\(\\beta_{0}\\) and their meaning is similar. \\(\\beta_{0}\\) is the log of the expected \\(y\\) when \\(x\\) is zero - the intercept. The log of \\(\\beta_{1}\\) is not the amount you add to \\(y\\) for each unit change in \\(x\\) but the amount by which multiply. This means the model is a curve. If \\(\\beta_{1}\\) is positive, \\(exp(\\beta_{1})\\) is greater than one and \\(y\\) increases as \\(x\\) increases; If \\(\\beta_{1}\\) is negative, \\(exp(\\beta_{1})\\) is less than one and \\(y\\) decreases as \\(x\\) increases. See Figure 9.1 for an illustration of the curve for positive and negative \\(\\beta_{1}\\). Data fitted with a Poisson GLM. Data fitted with a Poisson GLM. Figure 9.1: Data fitted with a Poisson GLM. See Figure 9.2 for a graphical representation of generalised linear model terms. A Generalised linear model with Poisson distributed errors. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) must be exponentiated to be interpreted on the scale of the response. When \\(x=0\\) we predict the number of \\(y\\) to be \\(exp(\\beta_{0})\\). For each unit of \\(x\\), the number of y changes by a factor of \\(exp(\\beta_{1})\\) A Generalised linear model with Poisson distributed errors. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) must be exponentiated to be interpreted on the scale of the response. When \\(x=0\\) we predict the number of \\(y\\) to be \\(exp(\\beta_{0})\\). For each unit of \\(x\\), the number of y changes by a factor of \\(exp(\\beta_{1})\\) Figure 9.2: A Generalised linear model with Poisson distributed errors. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) must be exponentiated to be interpreted on the scale of the response. When \\(x=0\\) we predict the number of \\(y\\) to be \\(exp(\\beta_{0})\\). For each unit of \\(x\\), the number of y changes by a factor of \\(exp(\\beta_{1})\\) "],
["summary.html", "Chapter 13 Summary", " Chapter 13 Summary what these models have in common: Responses must be independent if they are not, you need mised models. key points where to go next "],
["references.html", "References", " References "]
]
